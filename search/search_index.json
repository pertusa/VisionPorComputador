{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":"<p>Libro para las pr\u00e1cticas de la asignatura Visi\u00f3n por Computador (grado en Ingenier\u00eda Rob\u00f3tica de la Universidad de Alicante).</p> <p>Despliega el men\u00fa de la izquierda para consultar los apuntes.</p>"},{"location":"SUMMARY.html","title":"Summary","text":"<ul> <li>Instalaci\u00f3n de OpenCV</li> <li>T1 - Introducci\u00f3n a OpenCV</li> <li>T2 - Imagen digital y v\u00eddeo</li> </ul>"},{"location":"caracteristicas.html","title":"T6- Caracter\u00edsticas de imagen","text":"<p>En este tema aprenderemos a detectar y extraer caracter\u00edsticas \u00fatiles que describen una imagen.</p>"},{"location":"caracteristicas.html#descriptores-de-contorno","title":"Descriptores de contorno","text":"<p>Comenzaremos viendo las caracter\u00edsticas que se usan para describir contornos. Estas caracter\u00edsticas asumen que la imagen se ha binarizado previamente y tenemos el contorno de los objetos que queremos reconocer.</p>"},{"location":"caracteristicas.html#momentos","title":"Momentos","text":"<p>Una vez hemos extraido los contornos de una imagen, por ejemplo mediante la funci\u00f3n <code>findContours</code> cuya sintaxis vimos en el tema anterior, podemos calcular el momento de un contorno usando la funci\u00f3n <code>moments</code>:</p> <pre><code>momentos = cv.moments(contour)\n</code></pre> <p>Esta funci\u00f3n calcula todos los momentos del contorno. Para acceder a un momento determinado podemos indicar, por ejemplo: <code>momentos['m00']</code>. Estos son todos los momentos que devuelve la funci\u00f3n:</p> <pre><code>#\u00a0Momentos espaciales\nm00, m10, m01, m20, m11, m02, m30, m21, m12, m03\n# Momentos centrales\nmu20, mu11, mu02, mu30, mu21, mu12, mu03\n# Momentos centrales normalizados\nnu20, nu11, nu02, nu30, nu21, nu12, nu03\n</code></pre> <p>Como ves, no todos los momentos se calculan. Por ejemplo, como el momento <code>mu00</code> es igual a <code>m00</code>, OpenCV no lo extrae.</p> <p>Si quieres conocer m\u00e1s detalles sobre la funci\u00f3n <code>moments</code> puedes consultar este enlace.</p>"},{"location":"caracteristicas.html#momentos-de-hu","title":"Momentos de Hu","text":"<p>Los 7 momentos de Hu se calculan con la funci\u00f3n <code>HuMoments</code> a partir de los momentos centrales normalizados extra\u00eddos previamente con <code>moments</code>. Por ejemplo:</p> <pre><code>hu = cv.HuMoments(momentos)  #\u00a0El array hu contiene los 7 momentos de Hu\n</code></pre> <p>La funci\u00f3n matchShapes de OpenCV usa internamente estos momentos de Hu para comparar contornos, como veremos m\u00e1s adelante.</p>"},{"location":"caracteristicas.html#cadenas-de-freeman","title":"Cadenas de Freeman","text":"<p>En las primeras versiones de OpenCV se usaba la funci\u00f3n <code>approxChains</code> para extraer c\u00f3digos de cadena, pero a partir de la versi\u00f3n 3 OpenCV ha eliminado esta funcionalidad porque pr\u00e1cticamente no se utilizaban.</p>"},{"location":"caracteristicas.html#descriptor-shape-context-sc","title":"Descriptor Shape Context (SC)","text":"<p>Para extraer y comparar contornos con <code>Shape Context</code> podemos usar el siguiente c\u00f3digo:</p> <pre><code># Creamos una instancia de este descriptor\nmySC = cv.createShapeContextDistanceExtractor()\n\n# Lo aplicamos a dos contornos para obtener su distancia.\ndistance = mySC.computeDistance(contour1, contour2) \n</code></pre>"},{"location":"caracteristicas.html#ejercicio","title":"Ejercicio","text":"<p>En este ejercicio extraeremos una serie de descriptores de contorno a partir de im\u00e1genes binarizadas. El objetivo es encontrar las im\u00e1genes m\u00e1s similares a una imagen de referencia (a la que llamaremos <code>query</code>).</p> <p>Se pide completar el siguiente c\u00f3digo implementando los comentarios marcados con <code>TODO</code>. Guarda este programa con el nombre <code>contourDescriptors.py</code>. </p> <p>En el <code>main</code> se recibe el n\u00famero de una imagen (hay 20 en la carpeta) para usarla como <code>query</code>. Si no indicamos ning\u00fan par\u00e1metro, por defecto la <code>query</code> ser\u00e1 la imagen n\u00famero 3. Despu\u00e9s se extraen los descriptores de esta imagen y a continuaci\u00f3n se comparan con todos los descriptores obtenidos para el resto de im\u00e1genes, obteniendo una distancia (un valor de similitud) por cada descriptor implementado.</p> <pre><code>import os\nimport cv2 as cv\nimport argparse\nimport numpy as np\nimport math\n\ndef extraerDescriptores(image):\n    # Creamos un diccionario en el que guardar los valores calculados\n    imDescriptors = dict()\n\n    # Calculamos todos los contornos de la imagen  \n    allcontours, hierarchy = cv.findContours(image, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)\n\n    # Extraemos el mayor contorno de la imagen, del que obtendremos todos los descriptores:\n    contour = max(allcontours, key=cv.contourArea)\n\n    #\u00a0TODO: Guardamos el mayor contorno para el descriptor SC.\n    imDescriptors['contour'] = None\n\n    # TODO: Calculamos el perimetro\n    imDescriptors['perimetro'] = None\n\n    # TODO: Calculamos la compactaci\u00f3n\n    imDescriptors['compactacion'] = None\n\n    # TODO: Calculamos la elongacion\n    imDescriptors['elongacion'] = None\n\n    # TODO: Calculamos la rectangularidad. Para ello usamos el rect\u00e1ngulo rotado MRE que envuelve el contorno con un \u00e1rea m\u00ednima\n    imDescriptors['rectangularidad'] = None\n\n    # TODO: calculamos el \u00e1rea del cierre convexo (pista: funcion convexHull)\n    imDescriptors['areaCierreConvexo'] = None\n\n    # TODO: Calculamos el centroide (X,Y) y orientacion usando los momentos\n    imDescriptors['centroide'] = None\n    imDescriptors['orientacion'] = None\n\n    # TODO: Calculamos los Momentos de Hu\n    imDescriptors['Hu'] = None\n\n    return imDescriptors\n\n\ndef computarDistancias(qDescriptors, imDescriptors):\n    # Calcular y devolver la distancia entre qDescriptors e imDescriptors para las siguientes caracter\u00edsticas:\n\n    # TODO: Shape Context\n    dSC = 0\n\n    # TODO: Per\u00edmetro\n    dPer = 0\n\n    # TODO: Compactaci\u00f3n\n    dComp = 0\n\n    # TODO: Elongaci\u00f3n\n    dElong = 0\n\n    # TODO: Rectangularidad\n    dRect = 0\n\n    # TODO: Area del cierre convexo\n    dCierre = 0\n\n    # TODO: Distancia eucl\u00eddea del centroide\n    dCent = 0\n\n    # TODO: Orientacion\n    dOr = 0\n\n    #\u00a0TODO: Momentos de Hu (ver f\u00f3rmula que se describe debajo)\n    dHu = 0\n\n    print(' dSC = %.3f' % dSC)\n    print(' dPer = %.3f' % dPer)\n    print(' dComp = %.3f' % dComp)\n    print(' dElong = %.3f' % dElong)\n    print(' dRect = %.3f' % dRect)\n    print(' dCierre = %.3f' % dCierre)\n    print(' dCent = %.3f' % dCent)\n    print(' dOr = %.3f' % dOr)\n    print(' dHu = %.3f' % dHu)\n\n\ndef main(args):\n    # Procesamos par\u00e1metros de entrada\n    path = 'shape_sample'\n    idxQuery = args.indexQuery\n\n    # Leemos imagen consulta\n    queryName = os.path.join(path, str(idxQuery) + '.png')\n    query = cv.imread(queryName, cv.IMREAD_GRAYSCALE)\n\n    # Comprobamos que la imagen se ha podido leer\n    if query is None:\n        print('Error al cargar la imagen')\n        quit()\n\n    qDescriptors = extraerDescriptores(query)\n\n    # Para las otras im\u00e1genes, calculamos sus descriptores y los comparamos con los de la consulta\n    for i in range(0,20):\n        idxImg = i+1\n        #\u00a0Ignoramos esta imagen si es la misma que la de referencia\n        if (idxImg != idxQuery):\n            #\u00a0Leemos la imagen\n            imageName = os.path.join(path, str(idxImg) + '.png')\n            image = cv.imread(imageName, cv.IMREAD_GRAYSCALE)\n\n            # Extraemos sus caracter\u00edsticas y las comparamos con las de la query\n            print('---------')\n            print('Imagen', idxImg, ':')\n            imDescriptors = extraerDescriptores(image)\n            computarDistancias(qDescriptors, imDescriptors)\n\n    return 0\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description = 'Programa para calcular y comparar caracter\u00edsticas de contorno.')\n    parser.add_argument('--indexQuery', '-i', type=int, default=3)\n    args = parser.parse_args()\n    main(args)\n</code></pre> <p>En este ejercicio necesitar\u00e1s descargarte estas im\u00e1genes, que debes descomprimir en un directorio llamado <code>shape_sample</code>.</p> <p>Para extraer los descriptores puedes usar algunas de estas funciones de OpenCV.</p> <p>En el c\u00e1lculo de la arcotangente (para la orientaci\u00f3n) debes usar la funci\u00f3n atan2 de la librer\u00eda <code>math</code>.</p> <p>Para calcular la distancia entre dos n\u00fameros reales debemos usar el valor absoluto de su diferencia. </p> <p>En el caso del centroide (que es un punto con sus coordenadas u,v), deberemos calcular la distancia Eucl\u00eddea entre los dos puntos.</p> <p>Para calcular las distancias de los descriptores Hu debes usar las siguientes ecuaciones:</p> <p></p> <p>donde <code>A</code> y <code>B</code> son las dos im\u00e1genes a comparar, y <code>m</code> se define como:</p> <p></p> <p></p> <p>En este caso, <code>sign</code> es el signo (-1 si es negativo, 1 si es positivo, 0 si es 0), y <code>h</code> son los momentos Hu n\u00famero <code>i</code>. S\u00f3lo debe sumarse un momento <code>i</code> si sus componentes son mayores que un umbral <code>1.e-5</code>. Es decir, si el valor absoluto del descriptor Hu n\u00famero <code>i</code> es mayor de <code>1.e-5</code> en ambas im\u00e1genes (con que en una sea menor, no se considera). En python este logaritmo se calcula con la funci\u00f3n <code>math.log10</code>.</p> <p>Este es el mismo c\u00e1lculo que hace internamente el m\u00e9todo <code>matchShapes</code> de OpenCV (algoritmo <code>cv.CONTOURS_MATCH_I1</code>) para comparar contornos, pero en este ejercicio tendr\u00e1s que implementarlo a mano.</p> <p>La salida del programa debe ser como la siguiente:</p> <pre><code>---------\nImagen 1 :\n dSC = 0.617\n dPer = 1738.650\n dComp = 0.014\n dElong = 0.111\n dRect = 0.322\n dCierre = 119251.000\n dCent = 269.763\n dOr = 0.135\n dHu = 1.659\n---------\nImagen 2 :\n dSC = 4.456\n dPer = 1602.241\n dComp = 0.001\n dElong = 0.016\n dRect = 0.317\n dCierre = 106320.000\n dCent = 251.654\n dOr = 0.525\n dHu = 1.581\n---------\nImagen 4 :\n dSC = 76.421\n dPer = 24.444\n dComp = 0.004\n dElong = 0.283\n dRect = 0.054\n dCierre = 3276.500\n dCent = 78.218\n dOr = 0.125\n dHu = 1.417\n---------\nImagen 5 :\n dSC = 4.841\n dPer = 1556.140\n dComp = 0.021\n dElong = 0.180\n dRect = 0.015\n dCierre = 110849.000\n dCent = 259.311\n dOr = 0.224\n dHu = 0.229\n---------\n....\n</code></pre> <p>Revisa los resultados probando con distintas im\u00e1genes. Los resultados de dSC es posible que sean ligeramente distintos ya que los puntos iniciales se obtienen de forma aleatoria.</p> <p>Cuanto menor sea la distancia m\u00e1s deber\u00edan parecerse. Evidentemente, para comparar formas similares algunos descriptores obtendr\u00e1n mejores resultados que otros. </p>"},{"location":"caracteristicas.html#textura","title":"Textura","text":"<p>Los filtros de Gabor se implementan en OpenCV creando un kernel mediante la funci\u00f3n <code>getGaborKernel</code>, que despu\u00e9s puede convolucionarse con una imagen mediante <code>filter2D</code> como ocurre con cualquier otro filtro.</p> <pre><code>ksize = 32\nsigma = 1\ntheta = 0\nlambd = 1.0\ngamma = 0.02\npsi = 0\n\nkernel = cv.getGaborKernel((ksize,ksize), sigma, theta, lambd, gamma, psi)\n\ndst = cv.filter2D(img,-1,kernel)\n</code></pre> <p>Como puedes ver, un filtro de Gabor se puede construir con muchos par\u00e1metros, pero los principales son estos:</p> <ul> <li><code>ksize</code>: Tama\u00f1o del filtro</li> <li><code>sigma</code>: Desviaci\u00f3n t\u00edpica de la envolvente gaussiana</li> <li><code>theta</code>: Orientaci\u00f3n de las bandas paralelas de la funci\u00f3n Gabor</li> <li><code>lambd</code>: Longitud de onda de la se\u00f1al sinusoidal</li> </ul>"},{"location":"caracteristicas.html#hog","title":"HOG","text":"<p>En OpenCV podemos extraer el descriptor HOG mediante <code>HOGDescriptor</code>:</p> <pre><code>winSize = (32,16)\nblockSize = (8,8)\nblockStride = (4,4)\ncellSize = (4,4)\nnbins = 9\n\nhog = cv.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)\n</code></pre> <p>La ayuda de OpenCV es bastante incompleta para este descriptor y es mejor poner directamente en el c\u00f3digo <code>help(cv.HOGDescriptor)</code> para obtener m\u00e1s informaci\u00f3n. Por simplificar, los par\u00e1metros principales del constructor (aunque hay m\u00e1s) son estos:</p> <ul> <li><code>winSize</code>: Tama\u00f1o de la ventana.</li> <li><code>blockSize</code>: Tama\u00f1o del bloque.</li> <li><code>blockStride</code>: Desplazamiento del bloque.</li> <li><code>cellSize</code>: Tama\u00f1o de la celda.</li> <li><code>nbins</code>: N\u00famero de bins usados para calcular el histograma de gradientes.</li> </ul> <p>Tambi\u00e9n podemos crear un descriptor HOG con los valores que vienen por defecto:</p> <pre><code>hog = cv.HogDescriptor()\n# Equivalente a: cv.HOGDescriptor((64,128), (16,16), (8,8), (8,8), 9)\n</code></pre> <p></p> <p>Es necesario tener en cuenta que, a diferencia del algoritmo que hemos visto en teor\u00eda, en la implementaci\u00f3n de OpenCV hay una ventana que va movi\u00e9ndose por toda la imagen para calcular los descriptores HOG. Como hemos podido ver, el tama\u00f1o de la ventana por defecto es de 64x128 p\u00edxeles, lo cual significa que los objetos a detectar deben tener al menos ese tama\u00f1o. Si trabaj\u00e1ramos con resoluciones menores, deber\u00edamos cambiarlo.</p> <p>La longitud por defecto del vector HOG (que podemos ver usando el m\u00e9todo <code>hog.getDescriptorSize()</code>) es de 3.780 elementos por cada descriptor. </p> <p>Una vez creado el descriptor podemos aplicarlo a una imagen de esta forma:</p> <pre><code>winStride = (0,0)\npadding = (0,0)\n\ndescriptors = hog.compute(img, winStride, padding, locations)\n</code></pre> <p>La funci\u00f3n <code>compute</code> guarda en  el vector <code>locations</code> los puntos donde se han encontrado las  personas en la imagen, y en <code>descriptors</code> los valores del descriptor para cada punto. Para calcular esto se usa un sistema de detecci\u00f3n de peatones, que veremos en el tema 7.</p> <p>Si en lugar de extraer el descriptor queremos directamente hacer la detecci\u00f3n de personas en una imagen (que es lo  m\u00e1s habitual), se puede usar directamente este c\u00f3digo:</p> <pre><code># La siguiente instrucci\u00f3n inicializa un detector de personas.\n# No hemos visto este detector en teor\u00eda porque est\u00e1 basado en aprendizaje autom\u00e1tico (algo que veremos m\u00e1s adelante), pero s\u00ed que hemos visto el descriptor.\nhog.setSVMDetector(cv.HOGDescriptor_getDefaultPeopleDetector())\n\n# Aplicamos el detector sobre la imagen\nhog.detectMultiScale(img)\n</code></pre> <p>Para obtener m\u00e1s ayuda sobre las opciones de detectMultiScale puedes consultar este enlace. Puedes ver un ejemplo completo de detecci\u00f3n de peatones en v\u00eddeos usando HOG aqu\u00ed.</p> <p>Desafortunadamente en OpenCV no hay una forma sencilla de visualizar los gradientes del descriptor HOG, pero la librer\u00eda <code>scikit-image</code> s\u00ed que tiene funciones muy c\u00f3modas para calcular y visualizar HOG como puede verse en este c\u00f3digo de ejemplo que produce este resultado:</p> <p></p>"},{"location":"caracteristicas.html#caracteristicas-locales","title":"Caracter\u00edsticas locales","text":"<p>Las caracter\u00edsticas locales son f\u00e1ciles de obtener en OpenCV, ya que esta librer\u00eda implementa todo lo necesario para detectar keypoints y extraer sus correspondientes descriptores.</p>"},{"location":"caracteristicas.html#detector","title":"Detector","text":"<p>Ejemplo de detecci\u00f3n usando MSER:</p> <pre><code>import cv2 as cv\nimport argparse\nimport numpy as np\n\nparser = argparse.ArgumentParser(description = 'Programa para detectar keypoints con MSER')\nparser.add_argument('--imagen', '-i', type=str, default='lena.jpg')\nargs = parser.parse_args()\n\n#\u00a0Cargamos la imagen\nimg = cv.imread(args.imagen)\n\n# Comprobamos que la imagen se ha podido leer\nif img is None:\n    print('Error al cargar la imagen')\n    quit()\n\n# Hacemos una copia en escala de grises para calcular los keypoints\ngray = img.copy()\ngray = cv.cvtColor(gray, cv.COLOR_BGR2GRAY)\n\n# Creamos el detector\ndetector = cv.MSER_create()\n\n# Aplicamos el detector para obtener los keypoints\nkeypoints = detector.detect(gray, None)\n\n# Dibujamos los keypoints sobre la imagen. La \u00faltima opci\u00f3n es para que los c\u00edrculos salgan con su tama\u00f1o correspondiente.\noutput = cv.drawKeypoints(gray, keypoints, None, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\n# Visualizamos el resultado\ncv.imshow('Keypoints', output)\ncv.waitKey(0)\n</code></pre> <p>La salida ser\u00eda la siguiente:</p> <p></p> <p>Si remplazamos <code>MSER_create</code> por <code>SIFT_create</code> para usar una detector SIFT obtendremos el siguiente resultado:</p> <p></p>"},{"location":"caracteristicas.html#descriptor","title":"Descriptor","text":"<p>Veamos otro ejemplo, esta vez usando ORB como detector y tambi\u00e9n como descriptor. En este c\u00f3digo, adem\u00e1s comparamos los descriptores binarios usando una distancia Hamming. Las correspondencias que devuelve el m\u00e9todo <code>match</code> en la variable <code>matches</code> son las parejas de puntos m\u00e1s similares entre la primera y la segunda imagen.</p> <pre><code>import cv2 as cv\nimport argparse\nimport numpy as np\n\nparser = argparse.ArgumentParser(description = 'Programa para calcular descriptores MSER y compararlos')\nparser.add_argument('--queryImage', '-q', type=str, default='OSE1cor_1.png')\nparser.add_argument('--trainImage', '-t', type=str, default='OSE1cor_2.png')\n\nargs = parser.parse_args()\n\n#\u00a0Cargamos las im\u00e1genes en escala de grises\nimage1 = cv.imread(args.queryImage, cv.IMREAD_GRAYSCALE)\nimage2 = cv.imread(args.trainImage, cv.IMREAD_GRAYSCALE)\n\n# Comprobamos que se han podido leer\nif image1 is None or image2 is None:\n    print('Error al cargar la imagen')\n    quit()\n\n# Creamos el detector ORB con 100 puntos como m\u00e1ximo\norb = cv.ORB_create(100)\n\n# Usamos ORB para detectar los keypoints y calcular sus descriptores\nkeypoints1, descriptors1 = orb.detectAndCompute(image1, None)\nkeypoints2, descriptors2 = orb.detectAndCompute(image2, None)\n\n# Creamos el matcher y lo aplicamos\nbf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\nmatches = bf.match(descriptors1, descriptors2)\nprint('N\u00famero de matches encontrados:', len(matches))\n\n#\u00a0Dibujamos el resultado\nimageMatches = cv.drawMatches(image1, keypoints1, image2, keypoints2, matches, (255,255,255), (255,255,255))\ncv.imshow('ORB Matches', imageMatches)\n\ncv.waitKey(0)\n</code></pre> <p>Usando como entrada estas dos im\u00e1genes:</p> <p></p> <p></p> <p>El resultado de la ejecuci\u00f3n ser\u00eda el siguiente:</p> <p></p> <p>En este caso hemos utilizado el m\u00e9todo <code>detectAndCompute</code>, pero tambi\u00e9n pueden usarse por separado <code>detect</code> y <code>compute</code>.</p> <p>Al igual que en el ejemplo anterior, podemos remplazar ORB por SIFT para obtener las correspondencias con ese descriptor, pero el comparador no puede ser <code>NORM_HAMMING</code>  sino por ejemplo <code>NORM_L2</code> (distancia Eucl\u00eddea). </p> <p>En caso de que haya muchos puntos coincidentes (por ejemplo si no limitamos a 100 keypoints por imagen) normalmente queremos quedarnos s\u00f3lo con los mejores. Para esto podemos ordenar las coincidencias de menor a mayor distancia de la siguiente forma: </p> <pre><code>matches = sorted(matches, key = lambda x:x.distance)\n#\u00a0Nos quedamos con los 50 puntos que m\u00e1s se parecen\nimageMatches = cv.drawMatches(image1, keypoints1, image2, keypoints2, matches[:50], image2, flags=2)\n</code></pre> <p>En general, tenemos muchas combinaciones en OpenCV para usar detectores y descriptores, como puede consultarse en el siguiente listado.</p> <p>SIFT est\u00e1 patentado pero sus derechos han expirado en 2020 y por tanto ahora puede usarse sin problema en OpenCV. </p> <p>Sin embargo, SURF sigue con derechos de patente vigentes y desde OpenCV4.2 se dej\u00f3 fuera de la librer\u00eda ya que su filosof\u00eda es que todo lo que contenga sea de c\u00f3digo abierto. </p> <p>En este enlace puedes consultar muchos ejemplos de c\u00f3digo que usan detectores y descriptores de OpenCV.</p>"},{"location":"caracteristicas.html#descriptores-neuronales","title":"Descriptores neuronales","text":"<p>Como hemos visto en teor\u00eda, tambi\u00e9n podemos usar una red neuronal convolucional (CNN) para extraer una representaci\u00f3n vectorial de una imagen.</p> <p>Vamos a usar una red neuronal est\u00e1ndar de la librer\u00eda <code>Caffe</code> (una de las librer\u00edas que existen dedicadas a las redes neuronales profundas) en OpenCV para extraer descriptores neuronales:</p> <pre><code>import cv2 as cv\nimport argparse\nimport numpy as np\n\nparser = argparse.ArgumentParser(description = 'Programa para extraer descriptores neuronales')\nparser.add_argument('--imagen', '-i', type=str, default='space_shuttle.jpg')\n\nargs = parser.parse_args()\n\n#\u00a0Cargamos las im\u00e1genes en escala de grises\nimage = cv.imread(args.imagen)\n\n# Comprobamos que la imagen se ha podido leer\nif image is None:\n    print('Error al cargar la imagen')\n    quit()\n\n# Cargamos una red de Caffe.\nprotoFile = 'bvlc_googlenet.prototxt'\nweightsFile = 'bvlc_googlenet.caffemodel'\nnet = cv.dnn.readNetFromCaffe(protoFile, weightsFile)\n\n# Preparamos la imagen para la entrada de la red, que recibe un blob de un tama\u00f1o fijo (en este caso, 224x224)\ninWidth = 224\ninHeight = 224\ninputBlob = cv.dnn.blobFromImage(image, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n\n# Pasamos la imagen a la red\nnet.setInput(inputBlob)\n\n#\u00a0Hacemos una pasada forward hasta la capa de la cual queremos obtener los descriptores neuronales\nout = net.forward('pool5/7x7_s1')\n\n#\u00a0Convertimos la salida en un array unidimensional\nnc = out.flatten()\n\n# Lo imprimimos en una lista (para ver todos sus elementos)\nprint(nc.tolist())\n</code></pre> <p>Para poder usar este c\u00f3digo necesitaremos descargar los pesos de la red neuronal y la definici\u00f3n de su arquitectura.</p> <p>Si ejecutamos este c\u00f3digo se cargar\u00e1 una red de tipo GoogleNet ya entrenada con millones de im\u00e1genes de ImageNet. Dada una nueva imagen de entrada, esta se rescala y se pasa como entrada a la red neuronal. Escogemos como descriptor los valores de la pen\u00faltima capa que en este caso se llama <code>pool5/7x7_s1</code>.</p> <p>Con este programa ya tendremos nuestro descriptor neuronal que podemos usar como entrada a otra t\u00e9cnica de aprendizaje autom\u00e1tico como kNN o SVM.</p>"},{"location":"deteccion.html","title":"Tema 4 - Procesamiento de imagen: Filtrado y detecci\u00f3n de bordes, l\u00edneas, puntos aislados y esquinas","text":"<p>En este tema aprenderemos a detectar zonas de inter\u00e9s en im\u00e1genes.</p>"},{"location":"deteccion.html#deteccion-de-bordes","title":"Detecci\u00f3n de bordes","text":"<p>OpenCV tiene una funci\u00f3n que implementa directamente el gradiente de Sobel en ambas direcciones usando un kernel de tama\u00f1o 3x3 (este es el valor por defecto si no se indica ksize): </p> <pre><code># Gradiente X\nsobelx = cv.Sobel(img, cv.CV_64F, 1, 0, ksize=3)\n\n# Gradiente y\nsobely = cv.Sobel(img, cv.CV_64F, 0, 1, ksize=3)\n</code></pre>"},{"location":"deteccion.html#ejemplo-de-uso","title":"Ejemplo de uso:","text":"<pre><code>import cv2 as cv\nimport numpy as np\n\nimg = cv.imread('lena.jpg', cv.IMREAD_GRAYSCALE);\n\n# Comprobamos que la imagen se ha podido leer\nif img is None:\n    print('Error al cargar la imagen')\n    quit()\n\n#\u00a0Calculamos gradiente horizontal y vertical\ndx = cv.Sobel(img, cv.CV_64F, 1, 0)\ndy = cv.Sobel(img, cv.CV_64F, 0, 1)\n\n#\u00a0Calculamos la magnitud\nmagn = cv.magnitude(dx, dy)\n\n#\u00a0Convertimos de float a uint para poder mostrar el resultado\ndst = np.uint8(magn)\n\n# Mostramos el resultado\ncv.imshow('Sobel', dst)\ncv.waitKey(0)\n</code></pre> <p>Para convolucionar una imagen con un filtro hay que usar el m\u00e9todo <code>filter2D</code>. Ejemplo:</p> <pre><code>filtered = cv.filter2D(img, -1, kernel)\n</code></pre> <p>Donde kernel es la matriz que convolucionaremos con la imagen <code>img</code>. Hemos visto la sintaxis completa de este m\u00e9todo en el tema de transformaciones, aunque puedes consultar un ejemplo de uso en este otro enlace.</p> <p>Se puede usar OpenCV para extraer los gradientes en ambas direcciones de una imagen en escala de grises usando convoluciones mediante las f\u00f3rmulas vistas en teor\u00eda, y esto es lo que haremos en el siguiente ejercicio.</p>"},{"location":"deteccion.html#ejercicio","title":"Ejercicio","text":"<p>Podemos usar distintos kernels para implementar gradientes mediante convoluci\u00f3n. Crea un programa llamado <code>prewitt.py</code> a partir del siguiente c\u00f3digo, realizando las convoluciones correspondientes de los filtros Prewitt en horizontal y vertical y completando las partes indicadas con TODO:</p> <pre><code>import cv2 as cv\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser(description='Programa para calcular el filtro de Prewitt.')\nparser.add_argument('--imagen', '-i', type=str, default = 'lena.jpg')\nparser.add_argument('--salida', '-s', type=str, default = 'prewitt.jpg')\nargs = parser.parse_args()\n\n#\u00a0Cargamos la imagen\nimg = cv.imread(args.imagen, cv.IMREAD_GRAYSCALE)\n\n# Comprobamos que la imagen se ha cargado correctamente\nif img is None:\n    print('Error al cargar la imagen ')\n    quit()\n\n#\u00a0TODO: Obtenemos el gradiente horizontal mediante convoluci\u00f3n\n\n#\u00a0TODO: Obtenemos el gradiente vertical mediante convoluci\u00f3n\n\n# TODO: Pasamos a float ambos gradientes\n\n#\u00a0TODO: Obtenemos la magnitud y la guardamos en magn\n\n# Normalizamos para posteriormente poder convertir de float a uint sin tener valores fuera de rango\nmagn = magn-magn.min()\nmagn = magn/magn.max()*255\n\n#\u00a0TODO: Convertimos de float a uint para visualizar el resultado y lo guardamos en dst\n\n#\u00a0TODO: Guardamos dst en el fichero recibido como par\u00e1metro de salida\n\n# Mostramos el resultado\ncv.imshow('Prewitt', dst)\ncv.waitKey(0)\n</code></pre> <p>El programa debe leer la imagen de entrada en escala de grises, calcular la magnitud del gradiente, mostrarlo y guardarlo en la imagen pasada como par\u00e1metro. En el caso de lena.jpg deber\u00eda obtenerse la siguiente imagen:</p> <p></p> <p>Ver\u00e1s que el resultado de ejecutar ambos programas es distinto y que en este ejemplo particular los bordes se detectan mejor con Sobel que con Prewitt.</p>"},{"location":"deteccion.html#reduccion-de-ruido","title":"Reducci\u00f3n de ruido","text":"<p>Como hemos visto en teor\u00eda, los filtros Gausianos sirven para suavizar la imagen y eliminar ruido. Se suelen usar como paso previo a los sistemas de detecci\u00f3n de bordes para evitar que el ruido les afecte.</p> <p>En OpenCV podemos implementar un filtro Gaussiano mediante la funci\u00f3n <code>GaussianBlur</code>, que requiere que indiquemos el tama\u00f1o del filtro y su desviaci\u00f3n t\u00edpica:</p> <pre><code>dst = cv.GaussianBlur(src, (3,3), 0) # Realiza un filtrado gaussiano con un kernel de 3x3 p\u00edxeles y desviaci\u00f3n t\u00edpica 0\n</code></pre> <p>Esta funci\u00f3n admite m\u00e1s par\u00e1metros, como la desviaci\u00f3n t\u00edpica en el eje Y (si se omite es la misma que en el eje X) o el tipo de interpolaci\u00f3n en los bordes (por defecto, <code>cv.BORDER_DEFAULT</code>).</p> <p>Tambi\u00e9n podemos aplicar un filtro bilateral del siguiente modo:</p> <pre><code>dst = cv.bilateralFilter(src, 15, 80, 80) #  Aplica un filtro bilateral con un di\u00e1metro de 15 pixeles vecinos y una intensidad m\u00ednima 80.\n</code></pre> <p>Como ves, los \u00faltimos par\u00e1metros son dos umbrales en lugar de uno (es algo complicado de explicar, pero se usan para las im\u00e1genes en color). </p> <p>Normalmente se utiliza el mismo valor para ambos umbrales. Si es peque\u00f1o (&lt; 10), el filtro no tendr\u00e1 mucho efecto. Si es grande (&gt; 150) tendr\u00e1 un efecto fuerte, haciendo que la imagen tenga un estilo de c\u00f3mic (cartoon). Para m\u00e1s informaci\u00f3n se puede consultar la referencia de la funci\u00f3n.</p> <p>Por \u00faltimo, en OpenCV podemos usar un filtro Canny de la siguiente forma:</p> <pre><code>dst = cv.Canny(src, 100, 200) #\u00a0Filtro canny con los umbrales minimo y maximo (hysteresis) proporcionados\n</code></pre> <p>Para m\u00e1s informaci\u00f3n sobre Canny en OpenCV puedes consultar este enlace. Tal como ocurre con las funciones anteriores, los filtros Canny tambi\u00e9n pueden tener m\u00e1s par\u00e1metros.</p>"},{"location":"deteccion.html#ejercicio_1","title":"Ejercicio","text":"<p>Vamos a hacer un ejercicio usando todos los filtros anteriores. En este caso partiremos del siguiente c\u00f3digo que debes descargar, completando las instrucciones indicadas con TODO. Se trata de un ejercicio para cartoonizar una imagen. Llama al siguiente programa <code>cartoonize.py</code>.</p> <pre><code>import cv2 as cv\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser(description='Programa para cartoonizar una imagen.')\nparser.add_argument('--imagen', '-i', type=str, default = 'lena.jpg')\nparser.add_argument('--salida', '-s', type=str, default = 'cartoonized.jpg')\nargs = parser.parse_args()\n\n#\u00a0Cargamos la imagen\nimg = cv.imread(args.imagen)\n\n# Comprobamos que la imagen se ha podido cargar\nif img is None:\n    print('Error al cargar la imagen')\n    quit()\n\n########## BORDES ############\n\n# Aplicamos un filtro de mediana (cv.medianBlur) de tama\u00f1o 7x7 para suavizar la imagen\n#\u00a0TODO\n\n# Usamos Canny para detectar los bordes con un umbral en el rango [50, 150]\n#\u00a0TODO\n\n# Dilatamos los bordes usando la funci\u00f3n dilate con un filtro cuadrado de tama\u00f1o 2x2\n# TODO (guardar en imgCanny)\n\n# Escalamos los valores resultantes en el rango [0...1] y los invertimos. \n# Esta operaci\u00f3n impl\u00edcitamente convierte el resultado (imgCannyf) a float64\nimgCannyf = 1 - (imgCanny / 255)\n\n# Sobre el resultado anterior aplicamos un filtro gaussiano de 5x5 pixels con desviacion tipica 0\n# TODO: Guardar en bordesf\n\n# Mostramos los bordes\ncv.imshow('Bordes', bordesf)\n\n########## COLOR ############\n\n# Sobre la imagen original (img), aplicamos un filtro bilateral de di\u00e1metro 9 con umbrales 150 y 150 \n# TODO: Guardar en imgBF\n\n# Truncamos los colores. En este caso usamos un valor de 40, cuanto m\u00e1s alto m\u00e1s \"cartoonizado\" \ndiv = 40\nquantized = (imgBF // div) * div\n\n# Mostramos el resultado de color\ncv.imshow('Color', quantized)\n\n# Lo convertimos a float64 para las siguientes operaciones\n# TODO: Guardar en resultf\n\n########## UNI\u00d3N DE BORDES Y COLOR ############\n\n# Usamos merge para crear una imagen de 3 canales con los bordes\nimgCanny3c = cv.merge((bordesf, bordesf, bordesf))\n\n# Multiplicamos las matrices de color y bordes para obtener la imagen final\n# TODO\n\n# Convertimos el resultado anterior en una imagen de 8 bits (uint8)\n#\u00a0TODO: Guardar en result \n\n# Mostramos la imagen final y la guardamos\ncv.imshow('Result', result)\ncv.imwrite(args.salida, result)\n\ncv.waitKey(0)\n</code></pre> <p>El programa deber\u00eda obtener exactamente esta salida:</p> <p></p> <p>Pista: Si en alg\u00fan momento se muestran errores relacionados con los tipos de datos de las im\u00e1genes, puedes usar las siguientes <code>shape</code> y <code>dtype</code> para consultar de qu\u00e9 tama\u00f1o y tipo son:</p> <pre><code>print(imagen.shape, imagen.dtype)\n</code></pre>"},{"location":"deteccion.html#deteccion-de-lineas","title":"Detecci\u00f3n de l\u00edneas","text":"<p>La forma m\u00e1s sencilla para ejecutar la transformada de Hough para detectar l\u00edneas es la siguiente:</p> <pre><code>lines = cv.HoughLinesP(src, lines, rho, theta, threshold)\n</code></pre> <ul> <li><code>src</code>: Imagen de un canal en escala de grises (aunque realmente suele ser binaria, ya que Hough se usa tras applicar Canny).</li> <li><code>rho</code>: Resoluci\u00f3n de la distancia del acumulador (en p\u00edxeles).</li> <li><code>theta</code>: Resoluci\u00f3n del \u00e1ngulo del acumulador (en p\u00edxeles).</li> <li><code>threshold</code>: Umbral del acumulador. S\u00f3lo se devuelven aquellas l\u00edneas que tienen m\u00e1s votos que este umbral.</li> </ul> <p>El resultado se guarda en <code>lines</code>, que es un vector de l\u00edneas. A su vez, cada l\u00ednea es otro vector de 4 elementos <code>(x1, y1, x2, y2)</code>, donde <code>(x1,y1)</code> y <code>(x2, y2)</code> son los puntos extremos de la l\u00ednea.</p> <p>Adem\u00e1s de estos par\u00e1metros, hay otros dos opcionales: <code>minLineLength</code>, que indica la m\u00ednima longitud de una l\u00ednea para descartar los segmentos m\u00e1s cortos que esta longitud, y <code>maxLineGap</code>, que es el m\u00e1ximo salto permitido entre puntos de la misma l\u00ednea para enlazarlos.</p> <p>Como hemos visto, la funci\u00f3n <code>Hough</code> debe usarse siempre tras un detector de bordes. Por ejemplo:</p> <pre><code>edges = cv.Canny(src, 50, 200, None, 3)\nlines = cv.HoughLinesP(edges, 1, np.pi/180, 50, None, 50, 10)\n</code></pre> <p>Veamos un programa completo que usa <code>Hough</code> y muestra las l\u00edneas detectadas sobre la imagen:</p> <pre><code>import cv2 as cv\nimport argparse\nimport numpy as np\n\nparser = argparse.ArgumentParser(description = 'Programa para obtener las l\u00edneas usando la transformada de Douglas Peucker')\nparser.add_argument('--imagen', '-i', type=str, default='corrected.jpg')\nargs = parser.parse_args()\n\n#\u00a0Cargamos la imagen\nimg = cv.imread(args.imagen)\n\n# Comprobamos que la imagen se ha podido leer\nif img is None:\n    print('Error al cargar la imagen')\n    quit()\n\n# Detectamos bordes    \nedges = cv.Canny(img, 20, 100, 3)\ncv.imshow('Bordes', edges)\n\n# Ejecutamos Hough\nlines = cv.HoughLinesP(edges, 1, np.pi/180, 20, None, 10, 0)\n\n# Dibujamos las l\u00edneas resultantes sobre una copia de la imagen original\ndst = img.copy()\nif lines is not None:\n    for i in range(0, len(lines)):\n        l = lines[i][0]\n        cv.line(dst, (l[0], l[1]), (l[2], l[3]), (0,0,255), 2, cv.LINE_AA)\n\ncv.imshow('Lineas', dst)\ncv.waitKey(0)\n</code></pre> <p>La salida tras aplicar el filtro Canny:</p> <p></p> <p>Las l\u00edneas detectadas usando Hough:</p> <p></p> <p>La transformada de Hough tambi\u00e9n se puede utilizar para detecci\u00f3n de otras formas geom\u00e9tricas, por ejemplo  c\u00edrculos. A continuaci\u00f3n podemos ver un ejemplo de llamada a la funci\u00f3n <code>HoughCircles</code>:</p> <pre><code>circles = cv.HoughCircles(img,cv.HOUGH_GRADIENT, 1, 20,\n                            param1=50, param2=30, minRadius=0, maxRadius=0)\n</code></pre> <p>Puedes consultar la documentaci\u00f3n de HoughCircles para obtener m\u00e1s informaci\u00f3n sobre estos par\u00e1metros.</p> <p>La funci\u00f3n <code>approxPolyDP</code> aproxima una curva o un pol\u00edgono mediante otra curva/pol\u00edgono con menos v\u00e9rtices, de forma que la distancia entre ambas sea menor o igual que la precisi\u00f3n especificada. Se implementa usando el algoritmo de Douglas-Peucker:</p> <pre><code>closed = True\nepsilon = 0.1*cv.arcLength(contour, closed)\napprox = cv.approxPolyDP(contour, epsilon, closed)\n</code></pre> <p>El par\u00e1metro <code>epsilon</code> es la m\u00e1xima distancia del contorno al contorno aproximado, y <code>closed</code> indica si el contorno es o no cerrado.</p> <p>Esta funci\u00f3n suele usarse tras extraer los contornos de una imagen mediante la funci\u00f3n <code>findContours</code>, la cual veremos en detalle en el siguiente tema de segmentaci\u00f3n.</p>"},{"location":"deteccion.html#deteccion-de-puntos-aislados","title":"Detecci\u00f3n de puntos aislados","text":"<p>Como hemos visto en teor\u00eda, la Laplaciana es la derivada del gradiente y se puede usar para detectar puntos aislados. Puede implementarse mediante una convoluci\u00f3n con un kernel laplaciano, pero OpenCV proporciona directamente la funci\u00f3n Laplacian, que internamente llama a Sobel para calcular los gradientes. Ejemplo de uso:</p> <pre><code>ddepth = cv.CV_16S\nkernel_size = 3\ndst = cv.Laplacian(src, ddepth, ksize=kernel_size)\n</code></pre>"},{"location":"deteccion.html#deteccion-de-esquinas","title":"Detecci\u00f3n de esquinas","text":"<p>En OpenCV podemos detectar esquinas usando Harris mediante la funci\u00f3n <code>cornerHarris</code>. Necesita como entrada una imagen en escala de grises y adem\u00e1s los siguientes par\u00e1metros: el n\u00famero de p\u00edxeles vecinos a tener en cuenta, el tama\u00f1o del filtro (apertureSize) para calcular los gradientes con Sobel, y el umbral de detecci\u00f3n k, que es el \u00fanico par\u00e1metro libre del algoritmo Harris:</p> <pre><code>blockSize = 2 # Tama\u00f1o del vecindario considerado para la detecci\u00f3n de esquinas\napertureSize = 3 # Tama\u00f1o del kernel para el filtro de Sobel\nk = 0.04 #\u00a0Umbral de Harris\n\ndst = cv.cornerHarris(src, blockSize, apertureSize, k)\n</code></pre>"},{"location":"deteccion.html#ejercicio_2","title":"Ejercicio","text":"<p>Copia el siguiente c\u00f3digo, ll\u00e1malo <code>harris.py</code> y completa las instrucciones marcadas con TODO:</p> <pre><code>import cv2 as cv\nimport argparse\nimport numpy as np\n\nparser = argparse.ArgumentParser(description='Programa para calcular esquinas usando Harris.')\nparser.add_argument('--imagen', '-i', type=str, default = 'corrected.jpg')\nparser.add_argument('--salida', '-s', type=str, default = 'damasHarris.jpg')\nargs = parser.parse_args()\n\n#Importamos la imagen\nimg = cv.imread(args.imagen)\n\n# Comprobamos que la imagen se ha podido cargar\nif img is None:\n    print('Error al cargar la imagen')\n    quit()\n\n# Pasamos la imagen a escala de grises, y despu\u00e9s a float32\n#\u00a0TODO (guardar en img_gray)\n\n# Detectar las esquinas con Harris. Par\u00e1metros: blockSize=2, apertureSize=3, k=0.04.\n# TODO (guardar en dst)\n\n# Sobre la imagen original, poner en color azul los p\u00edxeles detectados como bordes.\n# Son aquellos que en los que dst(i,j) tiene un valor no inferior a 10000.\n# TODO (guardar en src).\n\n# Mostrar por pantalla la imagen src y adem\u00e1s guardarla en el fichero que se pasa al programa como segundo argumento\n# TODO\n</code></pre> <p>La imagen resultante debe ser como esta:</p> <p></p>"},{"location":"imagenvideo.html","title":"Tema 2- Imagen digital y v\u00eddeo","text":"<p>En este tema trabajaremos con cambios de resoluci\u00f3n de im\u00e1genes, histogramas, brillo/contraste y conversi\u00f3n entre espacios de color.</p>"},{"location":"imagenvideo.html#cambios-de-resolucion-espacial-y-radiometrica","title":"Cambios de resoluci\u00f3n espacial y radiom\u00e9trica","text":"<p>En OpenCV podemos cambiar la resoluci\u00f3n espacial (es decir, el tama\u00f1o de la imagen) con la funci\u00f3n <code>resize</code>:</p> <pre><code># Guardamos la imagen img en dst con el 75% de su tama\u00f1o original\ndst = cv.resize(img, (0,0), fx=0.75, fy=0.75)\n\n# Guardamos la imagen en destino con un tama\u00f1o de 640 x 480 (ancho x alto)\ndst = cv.resize(img, (640,480))\n</code></pre> <p>Para cambiar s\u00f3lo el tipo de dato de la imagen, que est\u00e1 relacionado con la resoluci\u00f3n radiom\u00e9trica o depth (el n\u00famero de bits usados para representar el valor de un p\u00edxel), podemos realizar una conversi\u00f3n con <code>numpy</code>:</p> <pre><code>converted = np.float32(img) #\u00a0Convertir a float y guardar en dst\n</code></pre>"},{"location":"imagenvideo.html#brillocontraste","title":"Brillo/contraste","text":"<p>Tal como puedes ver en las transparencias de teor\u00eda, el brillo y contraste de una imagen se definen con las siguientes ecuaciones:</p> <p></p> <p></p>"},{"location":"imagenvideo.html#ejercicio","title":"Ejercicio","text":"<p>Implementa un programa en OpenCV llamado <code>bc.py</code> que reciba por par\u00e1metro el nombre de una imagen (que debemos cargar en escala de grises) y muestre por el terminal su brillo y contraste. Los par\u00e1metros de entrada deben indicarse de la siguiente forma:</p> <pre><code>parser = argparse.ArgumentParser(description = 'Programa para obtener tanto el brillo como el contraste de una imagen.')\nparser.add_argument('--imagen', '-i', type=str, default='lena.jpg')\n</code></pre> <p>Ejemplo de ejecuci\u00f3n:</p> <pre><code>python bc.py\nb= 120.444\nc= 48.221\n</code></pre> <p>Importante: Como la correcci\u00f3n de pr\u00e1cticas es autom\u00e1tica, la salida del programa debe tener exactamente el formato que se indica, por lo que la \u00faltima l\u00ednea del c\u00f3digo deber\u00eda ser como la siguiente:</p> <pre><code>print('b= %.3f' % brillo)\nprint('c= %.3f' % contraste)\n</code></pre> <p>Las variables brillo y contraste son los valores que debes calcular usando las ecuaciones anteriores.</p>"},{"location":"imagenvideo.html#histogramas","title":"Histogramas","text":"<p>Podemos calcular el histograma de una imagen con la funci\u00f3n <code>calcHist</code>. Esta funci\u00f3n recibe muchos par\u00e1metros porque est\u00e1 pensada para calcular de una pasada todos los histogramas de un array de im\u00e1genes.</p> <p>Par\u00e1metros de <code>calcHist</code>:</p> <ul> <li><code>images</code>: Array de im\u00e1genes, deben tener la misma resoluci\u00f3n, tipo y el mismo tama\u00f1o, aunque pueden tener un n\u00famero distinto de canales. Tiene que indicarse entre corchetes, por ejemplo <code>[img]</code>.</li> <li><code>channels</code>: Tambi\u00e9n se da entre corchetes. Es la lista de los canales usados para calcular el histograma, comenzando por el n\u00famero de canal 0. Si es escala de grises se puede indicar <code>[0]</code>, y en color <code>[0]</code>, <code>[1]</code> o <code>[2]</code> para calcular los histogramas de azul, verde y rojo respectivamente.</li> <li><code>mask</code>: Matriz opcional para usar una m\u00e1scara binaria. Para obtener el histograma de la imagen completa se pone <code>None</code>.</li> <li><code>histSize</code>: Representa cu\u00e1ntos elementos tenemos en el vector del histograma y se pone entre corchetes, normalmente ser\u00e1n <code>[256]</code>. </li> <li><code>ranges</code>: Rango de los valores m\u00ednimos y m\u00e1ximos para cada imagen, normalmente es <code>[0,256]</code>.</li> </ul> <p>Veamos un ejemplo de c\u00f3digo que calcula el histograma de una imagen en escala de  grises, lo muestra por el terminal y crea una gr\u00e1fica en una ventana:</p> <pre><code>import cv2 as cv\nimport argparse\nfrom matplotlib import pyplot as plt\n\nparser = argparse.ArgumentParser(description = 'Programa para obtener el histograma de una imagen.')\nparser.add_argument('--imagen', '-i', type=str, default='lena.jpg')\nparser.add_argument('--histograma', '-o', type=str, default='histograma.png')\nargs = parser.parse_args()\n\n# Leemos la imagen indicada por el usuario\nimg = cv.imread(args.imagen,cv.IMREAD_GRAYSCALE) \n\n# Comprobamos la lectura de la imagen\nif img is None:\n    print(\"Error al leer la imagen \", args.imagen)\n    quit()\n\n# Obtenemos el histograma\nhist = cv.calcHist([img],[0],None,[256],[0,256])\n\n#\u00a0Lo mostramos usando la librer\u00eda matplotlib\nplt.plot(hist, 'b') #\u00a0El segundo par\u00e1metro es el color de la l\u00ednea ('b', 'g', o 'r')\nplt.xlim([0,256]) #\u00a0Para ajustar mejor el eje x y que s\u00f3lo se vean los valores en el rango [0,255].\n\n# Volcamos el resultado en un archivo\nplt.savefig(args.histograma)\n\n #\u00a0Mostramos el resultado por pantalla\nplt.show()\n</code></pre>"},{"location":"imagenvideo.html#ejercicio_1","title":"Ejercicio","text":"<p>Cuando la imagen es de tres canales, lo m\u00e1s normal es mostrar un histograma para cada uno de ellos. Haz una copia del programa anterior y ll\u00e1malo <code>histogramaColor.py</code>. Modif\u00edcalo para que en este caso el programa muestre en una ventana (y guarde en una imagen) el histograma de sus tres colores b\u00e1sicos, en lugar de hacerlo en escala de grises. </p> <p>Ejemplo de salida con <code>lena.jpg</code>:</p> <p></p>"},{"location":"imagenvideo.html#conversion-de-espacios-de-color","title":"Conversi\u00f3n de espacios de color","text":"<p>OpenCV soporta m\u00e1s de 150 espacios de color. La funci\u00f3n que realiza las conversiones entre ellos es <code>cvtColor</code>, y admite hasta 4 par\u00e1metros:</p> <ul> <li><code>src</code>: Imagen de entrada</li> <li><code>code</code>: C\u00f3digo de conversi\u00f3n del espacio de color. Su estructura es <code>COLOR_SPACEsrc2SPACEdst</code>. Ejemplos: <code>COLOR_BGR2GRAY</code>, <code>COLOR_YCrCb2BGR</code>.</li> <li><code>dst</code> (opcional): Imagen de salida con el mismo tama\u00f1o y resoluci\u00f3n (<code>depth</code>) que la imagen de entrada.</li> <li><code>dstCn</code> (opcional): El n\u00famero de canales en la imagen destino. Si se omite el par\u00e1metro, se infiere del n\u00famero de canales de la imagen <code>src</code> y de <code>code</code>.</li> </ul> <p>Ejemplo de uso:</p> <pre><code>grayImg = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n</code></pre> <p>Importante: La funci\u00f3n <code>cvtColor</code> s\u00f3lo convierte de BGR a otro espacio de color o viceversa, por lo que si queremos convertir una imagen entre dos espacios distintos de BGR, primero tenemos que pasarla a BGR y despu\u00e9s al espacio destino.</p> <p>Ejemplos de conversi\u00f3n:</p> <pre><code>converted = cv.cvtColor(img, cv.COLOR_BGR2GRAY) # Convertir a escala de grises\nconverted = cv.cvtColor(img, cv.COLOR_BGR2Luv) #\u00a0Convertir a LUV\nconverted = cv.cvtColor(img, cv.COLOR_BGR2XYZ) # Convertir a CIEXYZ\n</code></pre> <p>Puedes consultar en este enlace todas las ecuaciones que usa OpenCV para conversi\u00f3n entre espacios. Por ejemplo, para convertir un valor RGB en escala de grises se usa la siguiente f\u00f3rmula: 0.299*R + 0.587*G+ 0.114*B.</p> <p>Ten en cuenta que OpenCV a veces no usa el mismo orden de canales que el est\u00e1ndar del espacio de color. Por ejemplo, RGB se codifica en OpenCV como BGR, y HSL se codifica como HLS, por lo que el \u00faltimo canal que almacena en este caso es S en lugar de L.</p>"},{"location":"imagenvideo.html#ejercicio_2","title":"Ejercicio","text":"<p>Haz un programa llamado <code>colorLight.py</code> que reciba por par\u00e1metro el nombre de una imagen y extraiga los canales que se muestran a continuaci\u00f3n, guard\u00e1ndolos exactamente con el siguiente nombre de fichero:</p> <ul> <li>El canal L de CIELab, fichero <code>cielab_l.jpg</code>.</li> <li>El canal V de HSV, fichero  <code>hsv_v.jpg</code>.</li> <li>El canal L de HSL, fichero  <code>hsl_l.jpg</code>.</li> <li>El canal Y de YCrCb, fichero  <code>ycrcb_y.jpg</code>.</li> </ul> <p>Argumentos de entrada:</p> <pre><code>parser = argparse.ArgumentParser(description='Programa para cambiar entre espacios de color de una imagen.') \nparser.add_argument('--imagen', '-i', type=str, default = 'Fire_breathing_2_Luc_Viatour.jpg')\n</code></pre> <p>Para hacer pruebas puedes usar la siguiente imagen:</p> <p>Imagen de Luc Viatour, CC BY-SA 3.0, Wikimedia.</p> <p>Pista: Se puede usar el m\u00e9todo <code>split</code> para separar los canales de una imagen. Ejemplo:  <pre><code>b,g,r = cv.split(img)\n</code></pre> Alternativamente, tambi\u00e9n se puede usar <code>numpy</code> para obtener el canal que nos interese: <pre><code>b = img[:,:,0] #\u00a0Canal azul\ng = img[:,:,1] #\u00a0Canal verde\nr = img[:,:,2] #\u00a0Canal rojo\n</code></pre></p>"},{"location":"imagenvideo.html#pseudocolor","title":"Pseudocolor","text":"<p>Mediante la funci\u00f3n <code>applyColorMap</code> tambi\u00e9n podemos pseudocolorear im\u00e1genes en escala de grises usando los mapas de color predefinidos en OpenCV. Por ejemplo:</p> <pre><code>img = cv.imread('pluto.jpg', cv.IMREAD_GRAYSCALE)\nimgray = cv.applyColorMap(img, cv.COLORMAP_JET)\n</code></pre> <p>Escribe un programa completo (no hay que entregarlo) que contenga este c\u00f3digo de ejemplo para visualizar el resultado de pseudocolorear la siguiente imagen de Plut\u00f3n obtenida por la sonda New Horizons:</p> <p></p>"},{"location":"install.html","title":"Instalaci\u00f3n de OpenCV","text":"<p>OpenCV es la librer\u00eda m\u00e1s usada para procesamiento de im\u00e1genes, y puede instalarse en Windows, Linux, MacOS, iOS y Android. Tambi\u00e9n puede integrarse con ROS, lo que hace que OpenCV sea muy utilizado en el \u00e1mbito de la rob\u00f3tica. Est\u00e1 disponible para C++, Python y Java.</p> <p>En la asignatura de Visi\u00f3n por Computador usaremos OpenCV 4.8 en Python3. Para hacer las pr\u00e1cticas en los laboratorios trabajaremos en Linux (recomendado), aunque tambi\u00e9n puedes hacer los ejercicios en MacOS o en Windows.</p> <p>Hay dos opciones para instalar el software necesario para la asignatura, tanto en Linux como en MacOS o Windows: miniconda (recomendado) o instalaci\u00f3n directa.</p>"},{"location":"install.html#instalacion-con-miniconda-recomendado","title":"Instalaci\u00f3n con miniconda (recomendado)","text":"<p>Este tipo de instalaci\u00f3n hace que las librer\u00edas de python necesarias para la asignatura no interfieran con las versiones de otras librer\u00edas que ya teng\u00e1is instaladas en el sistema.</p> <p>Para esta opci\u00f3n deb\u00e9is instalar miniconda usando este enlace.</p> <p>Importante: Descargad la versi\u00f3n correspondiente a vuestro sistema operativo que ponga python 3.12. </p> <p>Si us\u00e1is Linux, desde un terminal hay que dar permisos de ejecuci\u00f3n al fichero descargado. Por ejemplo:</p> <pre><code>chmod +x ./Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Una vez tenemos los permisos podemos instalar conda: <pre><code>./Miniconda3-latest-Linux-x86_64.sh\n</code></pre></p> <p>Tras instalar conda debemos reiniciar el terminal. Si todo va bien, debe verse la palabra <code>(base)</code> al principio de la l\u00ednea de comandos.</p> <p>A continuaci\u00f3n hay que ejecutar (s\u00f3lo una vez) desde el terminal:</p> <pre><code> conda create -n vision python=3\n</code></pre> <p>Esto crea un entorno de python3. Cada vez que se inicie un nuevo terminal para hacer c\u00f3digo de Visi\u00f3n por Computador tendr\u00e1s que activar el entorno con esta instrucci\u00f3n:</p> <pre><code>conda activate vision\n</code></pre> <p>Una vez dentro del entorno se pueden instalar librer\u00edas de python (esto s\u00f3lo hay que hacerlo una vez, ya que quedan instaladas para dicho entorno) o tambi\u00e9n paquetes de linux con <code>apt-get</code>. Estos se instalar\u00e1n solo para el entorno:</p> <pre><code>pip3 install opencv-contrib-python numpy matplotlib pandas scikit-image scikit-learn\n</code></pre> <p>Se puede salir del entorno con el siguiente comando:</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"install.html#instalacion-directa","title":"Instalaci\u00f3n directa","text":"<p>Desde el terminal (con python3 previamente instalado) se puede ejecutar directamente:</p> <pre><code>pip3 install opencv-contrib-python numpy matplotlib pandas scikit-image scikit-learn\n</code></pre> <p>La ventaja es que no tendremos que cambiar de entorno cada vez que abrimos un nuevo terminal, pero el inconveniente de que si ten\u00e9is otras asignaturas (o en general otro software) que necesite versiones distintas de alguna de estas librer\u00edas puede haber problemas de compatibilidad.</p>"},{"location":"install.html#edicion-de-codigo-en-python","title":"Edici\u00f3n de c\u00f3digo en python","text":"<p>Se puede usar cualquier editor para python, pero se recomienda instalar Visual Studio Code. Este software est\u00e1 disponible para Linux, Mac y Windows.</p>"},{"location":"intro.html","title":"Tema 1- Introducci\u00f3n a OpenCV","text":"<p>En este tema veremos las funcionalidades b\u00e1sicas de OpenCV: cargar una imagen o un v\u00eddeo, mostrarlo por pantalla y guardar ficheros.</p>"},{"location":"intro.html#carga-y-visualizacion-de-imagenes","title":"Carga y visualizaci\u00f3n de im\u00e1genes","text":"<p>Vamos a comprobar que la instalaci\u00f3n se ha hecho de forma correcta ejecutando el siguiente programa de ejemplo.</p> <pre><code>import cv2 as cv\nimport argparse\n\n# Gesti\u00f3n de par\u00e1metros\nparser = argparse.ArgumentParser(description = 'Programa para cargar y mostrar una imagen')\nparser.add_argument('--imagen', '-i', type=str, default='lena.jpg')\nargs = parser.parse_args()\n\n# Cargamos la imagen indicada por el usuario (por defecto, en color)\nimg = cv.imread(args.imagen)\n\n# Comprobamos que la imagen se ha podido leer\nif img is None:\n    print(\"Error al cargar la imagen\", args.imagen)\n    quit()\n\n# Mostramos la imagen en una ventana con el t\u00edtulo \"Imagen\"\ncv.namedWindow('Ventana', cv.WINDOW_AUTOSIZE)\ncv.imshow('Ventana', img)\n\n#\u00a0Esperar a pulsar una tecla en la ventana para cerrarla\ncv.waitKey(0)\n</code></pre> <p>Primero guardamos este fichero con el nombre <code>lectura.py</code>. Para probar el c\u00f3digo, descargamos esta imagen de ejemplo:</p> <p></p> <p>Y a continuaci\u00f3n ejecutamos el programa de la siguiente forma:</p> <pre><code>python3 lectura.py\n</code></pre> <p>En el caso de Windows, en lugar de poner <code>python3</code> hay que escribir s\u00f3lo <code>python</code>.</p> <p>Como puede verse, se crea una ventana con la imagen, por defecto <code>lena.jpg</code>. Esta ventana se cerrar\u00e1 cuando pulsemos una tecla.</p> <p>Podemos ejecutar el programa con otra imagen indicando la opci\u00f3n <code>--imagen</code> o alternativamente la versi\u00f3n corta, <code>-i</code>:</p> <pre><code>python3 lectura.py --imagen otraimagen.jpg\n#\u00a0Equivalente a: python3 lectura.py -i otraimagen.jpg\n</code></pre> <p>Vamos a analizar este c\u00f3digo en detalle. En la primera l\u00ednea, el programa incluye OpenCV. Esto debemos hacerlo siempre que queramos usar la librer\u00eda. Tambi\u00e9n es conveniente incluir la librer\u00eda <code>numpy</code>, ya que es la que usa OpenCV para gestionar los arrays y por tanto nos permite acceder directamente a los valores de intensidad de las im\u00e1genes. A veces es posible que necesitemos incluir alg\u00fan fichero de cabecera adicional, como en este caso la librer\u00eda <code>argparse</code> que sirve para gestionar los argumentos del programa.</p> <p>La librer\u00eda <code>argparse</code> se encarga de comprobar que el usuario ha introducido los par\u00e1metros especificados. Si falta alg\u00fan par\u00e1metro se usan unos valores por defecto. Cuando un par\u00e1metro es opcional suele indicar que comience por <code>--</code>, aunque por defecto todos los par\u00e1metros son opcionales a no ser que se a\u00f1ada la opci\u00f3n <code>required=True</code>. A los par\u00e1metros sin valor por defecto se les asigna <code>None</code>, y esto es lo que recibe el programa si el usuario no establece su valor.</p> <p>A continuaci\u00f3n leemos una imagen con <code>imread</code> usando el nombre de fichero que se le pasa por par\u00e1metro y la guardamos en una matriz llamada <code>img</code>. Cada vez que intentemos cargar una imagen es importante comprobar que se ha podido leer  correctamente.</p> <p>En este punto ya tenemos la imagen cargada en una matriz. Podr\u00edamos procesarla, pero de momento s\u00f3lo vamos a mostrarla en una ventana. Para ello creamos una ventana con el nombre Ventana de tama\u00f1o variable, y llamamos al m\u00e9todo <code>imshow</code> para mostrar la imagen en la ventana.</p> <p>En este ejemplo la llamada a <code>namedWindow</code> puede omitirse sin consecuencias. Si <code>imshow</code> recibe como primer par\u00e1metro el nombre de una ventana que todav\u00eda no est\u00e1 creada, \u00e9sta se crea autom\u00e1ticamente con los par\u00e1metros por defecto.</p> <p>Siempre que mostremos una imagen en pantalla debemos llamar a la funci\u00f3n <code>waitKey(0)</code> para que la ventana se cierre cuando se pulse una tecla. Si no a\u00f1adimos esta l\u00ednea, la ventana no llegar\u00e1 a aparecer (mejor dicho, aparecer\u00e1 y se cerrar\u00e1 de inmediato).</p>"},{"location":"intro.html#carga-de-imagenes","title":"Carga de im\u00e1genes","text":"<p>La siguiente instrucci\u00f3n carga en una matriz la imagen cuyo nombre recibe por par\u00e1metro.</p> <pre><code>image = cv.imread('lena.jpg')\n</code></pre> <p>Como sabes, las im\u00e1genes digitales se representan con matrices.</p> <p></p> <p>Los formatos principales de im\u00e1genes soportadas por OpenCV son:</p> <ul> <li>Windows bitmaps (<code>bmp</code>, <code>dib</code>)</li> <li>Portable image formats (<code>pbm</code>, <code>pgm</code>, <code>ppm</code>)</li> <li>Sun rasters (<code>sr</code>, <code>ras</code>)</li> </ul> <p>Tambi\u00e9n soporta otros formatos a trav\u00e9s de librer\u00edas auxiliares:</p> <ul> <li>JPEG (<code>jpeg</code>, <code>jpg</code>, <code>jpe</code>)</li> <li>JPEG 2000 (<code>jp2</code>)</li> <li>Portable Network Graphics (<code>png</code>)</li> <li>TIFF (<code>tiff</code>, <code>tif</code>)</li> <li>WebP (<code>webp</code>).</li> </ul> <p>La funci\u00f3n <code>imread</code> tiene un par\u00e1metro opcional. Cuando carguemos una imagen en escala de grises debemos usar <code>IMREAD_GRAYSCALE</code>:</p> <pre><code># Cargamos la imagen (tanto si est\u00e1 en color como si no) en escala de grises\nimg = cv.imread('lena.jpg', cv.IMREAD_GRAYSCALE)\n</code></pre> <p>Esto es porque la opci\u00f3n por defecto es <code>IMREAD_COLOR</code>, y por tanto se cargar\u00e1 la imagen con 3 canales independientemente de que est\u00e9 o no en escala de grises. Estos son los tres tipos de opciones que podemos usar con <code>imread</code>:</p> <ul> <li><code>cv.IMREAD_GRAYSCALE</code>: Cargamos la imagen en escala de grises.</li> <li><code>cv.IMREAD_COLOR</code>: Cargamos la imagen en color. Si ten\u00eda canal alpha (transparente), se ignora.</li> <li><code>cv.IMREAD_UNCHANGED</code>: Cargamos la imagen tal como es incluyendo el canal alpha si lo tuviera.</li> </ul> <p>En general, cuando necesites ayuda sobre la sintaxis de cualquier funci\u00f3n de OpenCV, puedes escribir desde el mismo c\u00f3digo en python: <code>help(cv.imshow)</code>, reemplazando <code>imshow</code> por el nombre de la funci\u00f3n de la que deseas obtener ayuda.</p>"},{"location":"intro.html#imagenes-en-opencv","title":"Im\u00e1genes en OpenCV","text":"<p>En python, OpenCV usa arrays <code>numpy</code> para almacenar las im\u00e1genes. Para usar esta librer\u00eda en nuestro c\u00f3digo, tenemos que importarla al principio:</p> <pre><code>import numpy as np\n</code></pre> <p>Una vez importada podemos crear una matriz de cualquier tama\u00f1o, contenga o no los datos de una imagen:</p> <pre><code>img = np.full((100,100,3), (0,0,255), dtype=np.uint8)\nprint(img)\n</code></pre> <p>En este ejemplo hemos creado una matriz de 100x100x3 (podr\u00eda corresponder con una imagen de 100 filas por 100 columnas con 3 canales), inicializada con el valor rojo (0,0,255), y de tipo <code>uint8</code> (8 bits). Este tipo de dato es el est\u00e1ndar para crear im\u00e1genes con una profundidad de 8 bits en python (en el caso de C++ es <code>uchar</code> en lugar de <code>uint8</code>). Con 8 bits se pueden representar valores que van desde 0 a 255.</p> <p>Si visualizas esta imagen con <code>imshow</code> ver\u00e1s que es roja. Esto es porque el valor rojo se representa en el \u00faltimo canal debido a que OpenCV carga las im\u00e1genes en modo BGR en lugar del est\u00e1ndar RGB. Es decir, el canal 0 es el azul, el canal 1 el verde, y el canal 2 el rojo. </p> <p>Adem\u00e1s del tipo <code>uint8</code>, que es el m\u00e1s com\u00fan para im\u00e1genes, un array de numpy puede ser de cualquiera de estos tipos.</p> <p>Existen varias alternativas para asignar valores a una matriz que ya est\u00e1 creada, por ejemplo:</p> <pre><code>img.fill(255) # solo si la imagen es de 1 canal\nimg[::]=(255,0,0) #\u00a0Para cambiar todos los valores por (255,0,0)\n</code></pre> <p>Para m\u00e1s informaci\u00f3n sobre la sintaxis de acceso a arrays de <code>numpy</code> puedes consultar esta ayuda.</p> <p>En caso de que quisi\u00e9ramos inicializar todos los valores de la matriz a 0, a 1 o a cualquier otro valor tambi\u00e9n podr\u00edamos indicarlo:</p> <pre><code>img = np.zeros((100,100,3), dtype=np.uint8) #\u00a0Inicializaci\u00f3n con ceros\nimg = np.ones((100,100,3), dtype=np.uint8) #\u00a0Inicializaci\u00f3n con unos\n\nimg = np.array([[[255, 0, 0], [255, 0, 0]],\n                [[255, 0, 0], [255, 0, 0]],\n                [[255 ,0 ,0], [255, 0, 0]]], dtype=np.uint8) # Inicializaci\u00f3n de una matriz de tama\u00f1o 3x2x3 con todos los p\u00edxeles de color azul\n</code></pre> <p>En python, para copiar una variable en otra debemos llevar cuidado con usar el s\u00edmbolo igual. Por ejemplo:</p> <pre><code>x = img\ny = np.copy(img)\n</code></pre> <p>Si despu\u00e9s de ejecutar este c\u00f3digo modificamos <code>img</code> cambiar\u00e1 tambi\u00e9n el valor de <code>x</code> pero no el de <code>y</code>. Esto es porque el operador asignaci\u00f3n (<code>=</code>) no hace una copia de la matriz, sino que crea un puntero que apunta a la variable. Para hacer una copia es necesario el m\u00e9todo <code>copy</code>.</p> <p>Para acceder a valores individuales de una matriz podemos hacer uso de las siguientes opciones:</p> <pre><code>matrix = np.array([[1,2,3],[4,5,6]])    # Crea una matriz de 2x3\nprint(matrix[0, 0], matrix[0, 1], matrix[1, 0])   # Imprime \"1 2 4\"\nmatrix[0, 0] = 2 #\u00a0Cambia a 2 el valor de la posici\u00f3n 0,0\n</code></pre> <p>Si queremos obtener informaci\u00f3n sobre la estructura de la matriz podemos usar la siguiente instrucci\u00f3n:</p> <pre><code>print(matrix.dtype) # Imprime el tipo de la matriz\nprint(matrix.shape) # Imprime las dimensiones \"(2, 3)\"\nprint(matrix.ndim)  # Imprime el n\u00famero total de dimensiones (2)\nprint(matrix.size)  # Imprime el n\u00famero de elementos que tenemos en el array (6)\n</code></pre> <p>Podemos hacer multitud de operaciones con arrays <code>numpy</code>, tales como invertir matrices, transponerlas, etc.</p> <p>En numpy se puede seleccionar parte de un array de forma sencilla, como se puede ver en el siguiente ejemplo extraido de la ayuda de numpy: </p> <pre><code>data = np.array([1, 2, 3])\nprint(data[1])  #\u00a02\nprint(data[0:2]) #\u00a0(1,2)\nprint(data[1:])  #\u00a0(2,3)\nprint(data[-2:]) #\u00a0(2, 3)\n</code></pre> <p></p> <p>Para acceder de forma iterativa a todos los elementos de un array se puede usar el siguiente bucle:</p> <pre><code>for x in data:\n  print(x)\n</code></pre> <p>En el caso de una imagen para la que queramos iterar elemento a elemento:</p> <pre><code>for x in img:\n  for y in x:\n    print(y)\n</code></pre> <p>Si queremos acceder a una una imagen usando \u00edndices en lugar de iteradores se puede utilizar <code>range</code>:</p> <pre><code>rows,cols = img.shape\nfor i in range(rows): \n    for j in range(cols):  \n      print(img[i,j])\n</code></pre> <p>Tambi\u00e9n es posible crear una matriz que almacene una regi\u00f3n de inter\u00e9s (una zona rectangular) de otra imagen:</p> <pre><code>r = img[y1:y2, x1:x2]\n</code></pre> <p>Donde <code>(x1,y1)</code> son las coordenadas de la esquina superior izquierda del rect\u00e1ngulo que queremos recortar, y <code>(x2,y2)</code> son las coordenadas de la esquina inferior derecha.</p> <p>Puedes probar este programa de ejemplo para ver c\u00f3mo se extrae una subimagen, esta vez usando la funci\u00f3n <code>selectROI</code> que nos permite seleccionar una regi\u00f3n de inter\u00e9s mediante el interfaz de OpenCV:</p> <pre><code>import cv2 as cv\n\nimg = cv.imread('lena.jpg')\n\nr = cv.selectROI(img)\n\nimgCrop = img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]]\n\ncv.imshow('Crop', imgCrop)\ncv.waitKey(0)\n</code></pre> <p>A veces es necesario cambiar el tipo de dato de un array o matriz. Podemos hacer este cambio de forma sencilla usando los tipos <code>numpy</code>:</p> <pre><code>dst = np.float32(src) #\u00a0Conversi\u00f3n a float\ndst = np.intc(src) #\u00a0Conversi\u00f3n a int\ndst = np.uint8(src) #\u00a0Conversi\u00f3n a uint8\n</code></pre> <p>Hay veces en las que la conversi\u00f3n de tipos no puede hacerse directamente, por ejemplo cuando convertimos una matriz <code>float32</code>  en <code>uint8</code>, ya que podemos salirnos de rango (en el primer caso representamos la variable con 32 bits, en el segundo con 8). Para evitar esto se suele aplicar normalizaci\u00f3n. Por ejemplo, si tenemos una matriz <code>m</code> de tipo <code>float32</code> podemos hacer la conversi\u00f3n de la siguiente forma:</p> <pre><code>#\u00a0Normalizamos los valores entre 0 y 255\nm = m - m.min()\nm = m/m.max() * 255\n\n#\u00a0Ahora ya se puede convertir a uint8\ndst = np.uint8(m)\n</code></pre>"},{"location":"intro.html#guardar-imagenes","title":"Guardar im\u00e1genes","text":"<p>Para guardar una imagen en disco se usa la funci\u00f3n <code>imwrite</code> de OpenCV. Ejemplo:</p> <pre><code>cv.imwrite('output.jpg', img)\n</code></pre> <p>Esta funci\u00f3n determina el formato del fichero de salida a partir de la extensi\u00f3n proporcionada en su nombre (en este caso, JPG). Existe un tercer par\u00e1metro opcional en el que podemos indicar un array con opciones de escritura. Por ejemplo:</p> <pre><code>cv.imwrite('compress.png', img,  [cv.IMWRITE_PNG_COMPRESSION, 9]) #\u00a0Compresi\u00f3n PNG de nivel 9\n</code></pre> <p>Como hemos visto podemos guardar im\u00e1genes con <code>imwrite</code>, pero hay casos en los que esta operaci\u00f3n puede fallar (por ejemplo, cuando intentamos acceder a un directorio sin permisos). Si esto ocurre, el m\u00e9todo devolver\u00e1 <code>false</code>. Si queremos saber si se ha guardado correctamente la imagen, tenemos que comprobarlo (es recomendable hacerlo siempre):</p> <pre><code>writeStatus = cv.imwrite('img.jpg', img)\nif writeStatus is True:\n    print('Imagen guardada')\nelse:\n    print('Error al guardar la imagen') # Excepci\u00f3n u otro problema de escritura\n</code></pre>"},{"location":"intro.html#ejercicio","title":"Ejercicio","text":"<p>Haz un programa llamado <code>grayscale.py</code> que lea una imagen en color y la guarde en escala de grises. El programa recibir\u00e1 como argumento el nombre del fichero de la imagen en color y el del fichero en el que vamos a almacenar la misma imagen pero en escala de grises. Se proporciona la sintaxis de <code>argParse</code> para este ejercicio:</p> <pre><code>parser = argparse.ArgumentParser(description = 'Programa para cargar una imagen y guardarla en escala de grises')\nparser.add_argument('--entrada', '-i', type=str, default='lena.jpg')\nparser.add_argument('--salida', '-o', type=str, default='lenaGray.jpg')\n</code></pre> <p>Si la imagen no puede cargarse o guardarse, el programa debe imprimir el mensaje <code>Error al cargar la imagen</code> o <code>Error al guardar la imagen</code> respectivamente.</p>"},{"location":"intro.html#persistencia","title":"Persistencia","text":"<p>Adem\u00e1s de las funciones espec\u00edficas para leer y escribir im\u00e1genes y v\u00eddeo, en OpenCV hay otra forma gen\u00e9rica de guardar o cargar datos. Esto se conoce como persistencia de datos. Los valores de los objetos y variables en el programa pueden guardarse (serializados) en disco, lo cual es \u00fatil para almacenar resultados y cargar datos de configuraci\u00f3n.</p> <p>Estos datos suelen guardarse en un fichero <code>xml</code> mediante un diccionario (en algunos lenguajes de programaci\u00f3n como C++, a los diccionarios se les llama tambi\u00e9n mapas) usando pares clave/valor. Por ejemplo, si quisi\u00e9ramos guardar una variable que contiene el n\u00famero de objetos detectados en una imagen:</p> <pre><code>fs = cv.FileStorage('config.xml', cv.FileStorage_WRITE)\n#\u00a0Abrimos el fichero para escritura\nfs.write('numero_objetos', num_objetos) # Guardamos el numero de objetos\nfs.release() # Cerramos el fichero\n</code></pre> <p>Asumiendo que nuestra variable contiene el valor 10, se almacenar\u00e1 en disco el siguiente fichero <code>config.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;opencv_storage&gt;\n&lt;numero_objetos&gt;10&lt;/numero_objetos&gt;\n&lt;/opencv_storage&gt;\n</code></pre> <p>Si posteriormente queremos cargar esta informaci\u00f3n del fichero, podemos usar el siguiente c\u00f3digo:</p> <pre><code>fs = cv.FileStorage('config.xml', cv.FileStorage_READ)\nnum_objetos = fs.getNode('numero_objetos')\nfs.release()\n</code></pre>"},{"location":"intro.html#elementos-visuales","title":"Elementos visuales","text":"<p>Como hemos visto al principio, podemos crear una ventana para mostrar una imagen mediante la funci\u00f3n <code>namedWindow</code>. El segundo par\u00e1metro que recibe puede ser:</p> <ul> <li><code>cv.WINDOW_NORMAL</code>: El usuario puede cambiar el tama\u00f1o de la ventana una vez se muestra por pantalla.</li> <li><code>cv.WINDOW_AUTOSIZE</code>: El tama\u00f1o de la ventana se ajusta al tama\u00f1o de la imagen y el usuario no puede redimensionarla. Es la opci\u00f3n por defecto.</li> <li><code>cv.WINDOW_OPENGL</code>: Se crea la ventana con soporte para OpenGL (no es necesario en esta asignatura).</li> </ul> <p>Dentro de la ventana de OpenCV en la que mostramos la imagen podemos a\u00f1adir trackbars, botones, capturar la posici\u00f3n del rat\u00f3n, etc. En este enlace podemos ver los m\u00e9todos y constantes relacionados con la gesti\u00f3n del entorno visual est\u00e1ndar.</p> <p>Para capturar la posici\u00f3n del rat\u00f3n podemos usar el m\u00e9todo <code>setMouseCallback</code>, que recibe tres par\u00e1metros:</p> <ul> <li>El nombre de la ventana en la que se captura el rat\u00f3n.</li> <li>El nombre de la funci\u00f3n que se invocar\u00e1 cuando se produzca cualquier evento del rat\u00f3n (pasar por encima, clickar con el bot\u00f3n, etc).</li> <li>Un puntero (opcional) a cualquier objeto que queramos pasarle a nuestra funci\u00f3n.</li> </ul> <p>La funci\u00f3n <code>callback</code> que hemos creado recibe cuatro par\u00e1metros: El c\u00f3digo del evento, los valores <code>x</code> e <code>y</code>, unas opciones (flags) y el puntero al elemento pasado a la funci\u00f3n.</p> <pre><code>import cv2 as cv\n\n# Funci\u00f3n que se invoca cuando se usa el rat\u00f3n\ndef mouse_click(event, x, y, flags, param):\n    # En caso de que se pulse el bot\u00f3n izquierdo \n    if event == cv.EVENT_LBUTTONDOWN:\n\n        mensaje = 'Boton izquierdo (' + str(x) + ',' + str(y) + ')'\n\n        # Mostrar texto en la imagen.\n        cv.putText(img, mensaje, (x, y), cv.FONT_HERSHEY_TRIPLEX, 0.5, (255, 255, 255), 1) \n        cv.imshow('image', img)\n\n# Cargar imagen y mostrarla\nimg = cv.imread('lena.jpg') \ncv.imshow('image', img) \n\n#\u00a0Indicar la funci\u00f3n a llamar cuando se pulse el rat\u00f3n sobre la ventana\ncv.setMouseCallback('image', mouse_click)\n\ncv.waitKey(0)\n</code></pre> <p>Se puede encontrar m\u00e1s informaci\u00f3n sobre los par\u00e1metros de <code>putText</code> en este enlace.</p> <p>Mediante el m\u00e9todo <code>createTrackbar</code> podemos crear un trackbar (tambi\u00e9n llamado slider) para ajustar alg\u00fan valor en la ventana de forma interactiva. Al igual que ocurre con el m\u00e9todo que gestiona el rat\u00f3n, puede recibir como \u00faltimo par\u00e1metro una referencia a una funci\u00f3n (en el siguiente ejemplo, <code>onChange</code>):</p> <pre><code>import cv2 as cv\nimport argparse\n\n#\u00a0Constante para indicar el valor m\u00e1ximo del slider\nalpha_slider_max = 100\n\n# Funci\u00f3n que crea el trackbar\ndef onChange(val):\n    alpha = val / alpha_slider_max\n    beta = 1.0 - alpha\n    #\u00a0El m\u00e9todo addWeighted se encarga de hacer la mezcla\n    dst = cv.addWeighted(img1, alpha, img2, beta, 0.0)\n    cv.imshow('Linear Blend', dst)\n\n#\u00a0Procesamos argumentos\nparser = argparse.ArgumentParser(description='C\u00f3digo de ejemplo para usar un trackbar')\nparser.add_argument('--imagen1', help='Ruta de la primera imagen', default='LinuxLogo.jpg')\nparser.add_argument('--imagen2', help='Ruta de la segunda imagen', default='WindowsLogo.jpg')\nargs = parser.parse_args()\n\n#\u00a0Cargamos las imagenes y comprobamos que han podido abrirse\nimg1 = cv.imread(args.imagen1)\nimg2 = cv.imread(args.imagen2)\n\nif img1 is None:\n    print('No se ha podido abrir la imagen', args.imagen1)\n    quit()\nif img2 is None:\n    print('No se ha podido abrir la imagen', args.imagen2)\n    quit()\n\n# Creamos la ventana\ncv.namedWindow('Linear Blend')\n\n#\u00a0Creamos el trackbar\ncv.createTrackbar('Alpha', 'Linear Blend' , 0, alpha_slider_max, onChange)\n\n# Llamamos a la funci\u00f3n que gestiona lo que se hace cuando se modifica el trackbar\nonChange(0)\n\n# Esperamos a que el usuario pulse una tecla para salir\ncv.waitKey()\n</code></pre> <p>Necesitar\u00e1s estas dos im\u00e1genes para probar el c\u00f3digo:</p> <p> </p> <p>Como alternativa a usar los elementos visuales nativos de la interfaz de OpenCV, puedes usar otras librer\u00edas m\u00e1s potentes como imgui, aunque en principo no nos har\u00e1 falta para esta asignatura.</p>"},{"location":"intro.html#video","title":"V\u00eddeo","text":"<p>OpenCV permite cargar ficheros de v\u00eddeo o usar una webcam para realizar procesamiento en tiempo real. Veamos un ejemplo de detecci\u00f3n de bordes usando una webcam (dar\u00e1 un error al ejecutarlo si el laboratorio no est\u00e1 equipado con c\u00e1maras, aunque si tienes un port\u00e1til puedes probarlo):</p> <pre><code>import cv2 as cv\n\ncap = cv.VideoCapture(0)\n\nwhile(True):\n    # Capturar frame a frame\n    ret, frame = cap.read()\n\n    # Aqu\u00ed podemos procesar los frames\n    if ret:\n        edges = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n        edges = cv.GaussianBlur(edges, (7,7), 1.5, 1.5);\n        edges = cv.Canny(edges, 0, 30, 3);\n\n        # Mostrar el resultado\n        cv.imshow('frame',edges)\n    else:\n        break\n\n    #\u00a0Parar cuando el usuario pulse 'q'\n    if cv.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\n# Cuando terminemos, paramos la captura\ncap.release()\n</code></pre> <p>Como puede verse, el c\u00f3digo es bastante sencillo. Simplemente tenemos que inicializar una variable de captura de v\u00eddeo, y con <code>read</code> podemos obtener los frames para procesarlos. Si <code>ret</code> es <code>True</code> es porque el frame se ha podido leer correctamente.</p> <p>En caso de que queramos cargar un fichero de v\u00eddeo (por ejemplo, este ), s\u00f3lo hay que cambiar un par de l\u00edneas:</p> <pre><code>cap = cv.VideoCapture('Megamind.avi')\n\nwhile(cap.isOpened()):\n</code></pre> <p>Para guardar un fichero de v\u00eddeo hay que llamar a la funci\u00f3n <code>VideoWriter</code> especificando el formato, fps (frames por segundo) y las dimensiones. Por ejemplo:</p> <pre><code>out = cv.VideoWriter('output.avi', fourcc, 20.0, (640,480)) #\u00a0 AVI, 20fps, 640x480\n</code></pre> <p>Si intentas guardar directamente el v\u00eddeo resultante del programa anterior con VideoWriter no funcinar\u00e1 porque los bordes est\u00e1n en escala de grises y todos los formatos admitidos de v\u00eddeo necesitan frames en color.</p> <p>Estos son algunos de los formatos aceptados, aunque existen muchos m\u00e1s:</p> <pre><code>fourcc = cv.VideoWriter_fourcc('m','j','p','g') # AVI, recomendado en la asignatura\nfourcc = cv.VideoWriter_fourcc('d','i','v','3') # DivX MPEG-4 codec\nfourcc = cv.VideoWriter_fourcc('m','p','e','g') # MPEG-1 codec\nfourcc = cv.VideoWriter_fourcc('m','p','g','4') # MPEG-4 codec\nfourcc = cv.VideoWriter_fourcc('d','i','v','x') # DivX codec\n</code></pre> <p>Para escribir un frame de v\u00eddeo podemos usar el m\u00e9todo <code>write</code>:</p> <pre><code>out.write(frame)\n</code></pre>"},{"location":"reconocimiento.html","title":"T7- Reconocimiento","text":""},{"location":"reconocimiento.html#t7-reconocimiento-de-imagen","title":"T7- Reconocimiento de imagen","text":"<p>En este tema aprenderemos a buscar im\u00e1genes similares y a reconocer la clase de una imagen.</p>"},{"location":"reconocimiento.html#busqueda-por-similitud","title":"B\u00fasqueda por similitud","text":"<p>Como hemos visto en teor\u00eda, para encontrar im\u00e1genes similares podemos extraer y comparar descriptores usando distintas m\u00e9tricas de distancia.</p>"},{"location":"reconocimiento.html#descriptores-binarios","title":"Descriptores binarios","text":"<p>En el tema anterior vimos un ejemplo de c\u00f3digo para comparar im\u00e1genes (<code>matching</code>) usando ORB y la distancia de Hamming. Rev\u00edsalo antes de continuar con este tema. </p> <p>La t\u00e9cnica que hab\u00edamos usado en ese ejemplo (tambi\u00e9n llamada fuerza bruta) funciona bien con ORB porque es un descriptor binario y la comparaci\u00f3n es muy eficiente al ser simplemente una operaci\u00f3n XOR.</p> <p>Los descriptores binarios se idearon para hacer <code>matching</code> y funcionan mucho peor cuando se usan en problemas de reconocimiento (clasificaci\u00f3n), aunque a veces se emplean tambi\u00e9n para esta tarea por eficiencia.</p>"},{"location":"reconocimiento.html#descriptores-locales-basados-en-puntos-de-interes","title":"Descriptores locales basados en puntos de inter\u00e9s","text":"<p>Sin embargo, tal como hemos visto en teor\u00eda, comparar descriptores como SIFT o SURF no es r\u00e1pido, sobre todo si tenemos muchas im\u00e1genes en nuestra base de datos.</p> <p>Para comparar dos im\u00e1genes que tienen descriptores basados en puntos de inter\u00e9s (y, por tanto, un n\u00famero variable de elementos por imagen) se puede usar una t\u00e9cnica de vecinos m\u00e1s cercanos aproximados (en ingl\u00e9s, Approximate Nearest Neighbors). \u00c9sta consiste en construir una representaci\u00f3n interna para evitar hacer las comparaciones de todos los puntos con todos, mirando s\u00f3lo aquellos que pueden ser m\u00e1s similares. En OpenCV tenemos una funci\u00f3n que hace esta tarea: <code>FLANN</code>.</p> <p>Podemos ver un ejemplo completo de c\u00f3digo a continuaci\u00f3n: </p> <pre><code>import cv2 as cv\nimport argparse\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nparser = argparse.ArgumentParser(description = 'Programa para detectar keypoints con SIFT y compararlos con FLANN')\nparser.add_argument('--queryImage', '-q', type=str, default='box.png')\nparser.add_argument('--trainImage', '-t', type=str, default='box_in_scene.png')\n\nargs = parser.parse_args()\n\n# Cargamos las im\u00e1genes en escala de grises\nimage1 = cv.imread(args.queryImage, cv.IMREAD_GRAYSCALE)\nimage2 = cv.imread(args.trainImage, cv.IMREAD_GRAYSCALE)\n\n# Comprobamos que se han podido leer\nif image1 is None or image2 is None:\n    print('Error al cargar la imagen')\n    quit()\n\n# Creamos el descriptor SIFT con sus valores por defecto\nsift = cv.SIFT_create()\n\n# Usamos SIFT para detectar los keypoints y calcular sus descriptores\nkeypoints1, descriptors1 = sift.detectAndCompute(image1, None)\nkeypoints2, descriptors2 = sift.detectAndCompute(image2, None)\n\n# Hacemos el matching con FLANN utilizando 2 vecinos\nmatcher = cv.FlannBasedMatcher()\nmatches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n\n#\u00a0Nos quedamos s\u00f3lo los matches \"buenos\" y los guardamos en good\n# En el art\u00edculo original de SIFT, si dos puntos tienen una distancia menor de 0.7 se consideran un match\ngood = list()\nfor m, n in matches:\n    if m.distance &lt; 0.7 * n.distance:\n        good.append(m)\n\n#\u00a0Dibujamos el resultado\ndraw_params = dict(matchColor = (0,255,0), singlePointColor = (255,0,0))\n\nimageMatches = cv.drawMatches(image1, keypoints1, image2, keypoints2, good, None, **draw_params)\n\nplt.imshow(imageMatches)\nplt.show()\n</code></pre> <p>Usando como entrada estas dos im\u00e1genes:</p> <p> </p> <p>Se obtiene el siguiente resultado:</p> <p></p> <p>Como puedes ver, el libro de la imagen query se detecta bastante bien en la imagen train. </p> <p>El c\u00f3digo que realiza la comparaci\u00f3n de los descriptores correspondientes a cada punto de inter\u00e9s es el siguiente:</p> <pre><code>matcher = cv.FlannBasedMatcher()\nmatches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n</code></pre> <p>En este caso, <code>descriptors_1</code> contiene todos los descriptores de la primera imagen y <code>descriptors_2</code> los de la segunda. <code>FlannBasedMatches</code> construye un sistema de b\u00fasqueda de vecinos m\u00e1s cercanos aproximados usando estos descriptores de entrada, y luego usa estos vecinos para encontrar las mejores correspondencias entre los descriptores de ambas im\u00e1genes. </p> <p>Como puedes ver en el c\u00f3digo, se suelen eliminar los pares de puntos cuya distancia es mayor de un cierto umbral.</p>"},{"location":"reconocimiento.html#reduccion-de-dimensionalidad","title":"Reducci\u00f3n de dimensionalidad","text":"<p>Para hacer que el <code>matching</code> sea m\u00e1s eficiente (aunque normalmente a costa de empeorar un poco los resultados) podemos reducir el tama\u00f1o de los descriptores usando alguna de las siguientes t\u00e9cnicas que hemos visto en teor\u00eda:</p>"},{"location":"reconocimiento.html#bag-of-words-bow","title":"Bag of Words (BoW)","text":"<p>Usar BoW es mucho m\u00e1s eficiente que emplear descriptores completos cuando se trata de caracter\u00edsticas como SIFT, HOG o SURF (no binarias).</p> <p>Usando el siguiente c\u00f3digo podemos entrenar un diccionario BoW a partir los descriptores SIFT extra\u00eddos de todas las im\u00e1genes de un conjunto de entrenamiento:</p> <pre><code># Creamos una instancia BOW, en este caso el vocabulario tendr\u00e1 100 palabras\nBOW = cv.BOWKMeansTrainer(100)\nsift = cv.SIFT_create()\n\n#\u00a0Recorremos todas las im\u00e1genes extrayendo descriptores SIFT y a\u00f1adi\u00e9ndolos para poder entrenar nuestro BOW.\nfor file in imagesPath:\n    image = cv.imread(file, cv.IMREAD_GRAYSCALE)\n    keypoints, descriptors= sift.detectAndCompute(image, None)\n    BOW.add(descriptors)\n\n# Entrenamos para obtener el vocabulario\nvocabulary = BOW.cluster()\n</code></pre> <p>F\u00edjate que deber\u00e1s incluir la ruta a una o m\u00e1s im\u00e1genes en la variable <code>imagesPath</code> para poder ejecutar el c\u00f3digo. Puedes, por ejemplo, incluir las del ejercicio anterior (box.png y box_in_scene.png).</p> <p>Una vez ejecutado el c\u00f3digo, habremos entrenado un diccionario de <code>k=100</code> palabras. A continuaci\u00f3n podemos extraer un descriptor, y convertirlo en un histograma de palabras (este ser\u00e1 nuestro nuevo descriptor). </p> <pre><code>#\u00a0Inicializamos el extractor, que estar\u00e1 basado en SIFT y que asignar\u00e1 clusters por fuerza bruta\nBOWExtractor = cv.BOWImgDescriptorExtractor(sift, cv.BFMatcher(cv.NORM_L2))\n\n#\u00a0Asignamos al extractor declarado el vocabulario que hab\u00edamos entrenado\nBOWExtractor.setVocabulary(vocabulary)\n\n# Ahora ya podemos extraer el histograma BOW de una imagen\nBOWdescriptor = BOWExtractor.compute(image, sift.detect(image))\n\n# Mostramos el histograma:\nplt.hist(BOWdescriptor[0], 100)\nplt.show()\n</code></pre> <p>Puedes ver ejemplos de c\u00f3digo completos de entrenamiento y reconocimiento con BOW en este enlace y este otro.</p>"},{"location":"reconocimiento.html#pca","title":"PCA","text":"<p>Podemos ver un ejemplo completo de reducci\u00f3n de dimensionalidad mediante PCA en este enlace. La parte interesante est\u00e1 en la funci\u00f3n <code>getOrientation</code>:</p> <pre><code># Perform PCA analysis\nmean = np.empty((0))\nmean, eigenvectors, eigenvalues = cv.PCACompute2(data_pts, mean)\n</code></pre> <p>Como ves, PCA es un algoritmo de aprendizaje no supervisado, es decir, no necesita que las muestras est\u00e9n  etiquetadas. </p> <p>Aunque puede usarse OpenCV para calcular PCA, es m\u00e1s recomendable emplear <code>scikit-learn</code> como puede verse en este ejemplo, en el que el c\u00f3digo destacable para PCA es el siguiente:</p> <pre><code>from sklearn.decomposition import PCA\n\npca = PCA(2) # 2 componentes principales.\nconverted_data = pca.fit_transform(digits.data)\n</code></pre> <p><code>sklearn</code> es la forma de indicar en python la librer\u00eda <code>scikit-learn</code>.</p> <p>Tampoco entraremos en detalles sobre estas t\u00e9cnicas de reducci\u00f3n de dimensionalidad, ya que las ver\u00e9is en otra asignatura del grado, pero pod\u00e9is usarlas en vuestro proyecto para extraer descriptores m\u00e1s compactos.</p>"},{"location":"reconocimiento.html#deteccion-de-caras","title":"Detecci\u00f3n de caras","text":"<p>A continuaci\u00f3n puedes ver un ejemplo para detectar caras y ojos basado en este c\u00f3digo.</p> <pre><code>import numpy as np\nimport cv2 as cv\n\n#\u00a0Cargamos los modelos\nface_cascade = cv.CascadeClassifier('haarcascade_frontalface_default.xml')\neye_cascade = cv.CascadeClassifier('haarcascade_eye.xml')\n\n# Leemos la imagen\nimg = cv.imread('lena.jpg')\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n\n#\u00a0Ejecutamos el detector de caras\nfaces = face_cascade.detectMultiScale(gray)\n\nfor (x,y,w,h) in faces:\n    #\u00a0Dibujamos las caras\n    img = cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n\n    # Ejecutamos el detector de ojos en la zona de la cara\n    roi_gray = gray[y:y+h, x:x+w]\n    eyes = eye_cascade.detectMultiScale(roi_gray)\n\n    #\u00a0Dibujamos los ojos\n    roi_color = img[y:y+h, x:x+w]\n    for (ex,ey,ew,eh) in eyes:\n        cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n\ncv.imshow('Caras',img)\ncv.waitKey(0)\n</code></pre> <p>Prueba este programa descargando los modelos entrenados con descriptores Haar. El resultado con la imagen <code>lena.jpg</code> deber\u00eda ser el siguiente:</p> <p></p> <p>Tambi\u00e9n puedes usar descriptores LBP descargando sus correspondientes modelos desde este otro enlace.</p> <p>Como puedes ver en los modelos Haar disponibles en el enlace anterior, es posible usar el mismo c\u00f3digo para detectar matr\u00edculas, peatones, gatos, etc. simplemente cambiando el modelo.</p> <p>Alternativamente a OpenCV, tambi\u00e9n puedes usar la librer\u00eda <code>scikit-image</code> para detectar caras.</p> <p>En OpenCV puedes usar cualquier tipo de im\u00e1genes para entrenar tu propio modelo siguiendo los pasos que se indican en este enlace, aunque no es f\u00e1cil. Para esto deber\u00e1s instalar la versi\u00f3n de C++ de OpenCV y compilar varios programas: <code>opencv_createsamples</code>, <code>opencv_annotation</code>, <code>opencv_traincascade</code> y <code>opencv_visualisation</code>.</p> <p>Sin embargo, tambi\u00e9n puedes usar la librer\u00eda <code>scikit-image</code>, con la cual se simplifica bastante el entrenamiento como puede verse en este ejemplo.</p>"},{"location":"reconocimiento.html#reconocimiento-de-caras","title":"Reconocimiento de caras","text":"<p>Para reconocer caras (es decir, identificar a qu\u00e9 persona pertenecen) lo m\u00e1s f\u00e1cil es usar una librer\u00eda en python llamada face_recognition y seguir este tutorial.</p>"},{"location":"reconocimiento.html#clasificacion","title":"Clasificaci\u00f3n","text":"<p>Como hemos visto en teor\u00eda y al principio de este cap\u00edtulo una forma f\u00e1cil de clasificar una imagen es buscando im\u00e1genes similares que est\u00e9n etiquetadas y devolviendo la clase de la imagen m\u00e1s similar.</p>"},{"location":"reconocimiento.html#ejercicio","title":"Ejercicio","text":"<p>Vamos a hacer un ejercicio en el que extraeremos un descriptor ORB de una imagen y lo compararemos con los de otras im\u00e1genes ya etiquetadas para obtener su clase.</p> <p>Para esto tenemos que descargar un subconjunto de im\u00e1genes etiquetadas de la base de datos MirBot. MirBot es un proyecto desarrollado en la UA y consiste en un sistema de reconocimiento interactivo de im\u00e1genes para m\u00f3viles. Cuanto m\u00e1s usuarios y m\u00e1s fotos se a\u00f1ad\u00edan mejor funcionaba, aunque actualmente su desarrollo est\u00e1 descontinuado.</p> <p>Para este caso s\u00f3lo vamos a usar un subconjunto de las im\u00e1genes enviadas por los usuarios, en concreto algunas pertenecientes a estas 10 clases: book, cat, cellphone, chair, dog, glass, laptop, pen, remote, tv. Descomprime el fichero descargado y echa un vistazo para ver los casos que intentamos reconocer.</p> <p>Ahora se trata de completar los puntos marcados con <code>TODO</code> en el siguiente c\u00f3digo para realizar la clasificaci\u00f3n. Gu\u00e1rdalo con el nombre <code>orbBF.py</code> e intenta entender bien todas las instrucciones antes de empezar a modificarlo.</p> <pre><code>import cv2 as cv\nimport argparse\nimport numpy as np\nimport pickle\n\n# Funci\u00f3n para leer un fichero de texto con nombres de fichero de las im\u00e1genes y sus etiquetas\n#\u00a0Devuelve los descriptores calculados y las etiquetas leidas\ndef readData(filename):\n\n    descriptors = list()\n    labels = list()\n\n    with open(filename,'r') as f:\n\n        print('Cargando', filename,'...')\n\n        # TODO: Creamos el detector ORB con 100 puntos como m\u00e1ximo\n\n        for line in f.readlines():\n            #\u00a0Separamos nombre de imagen y etiqueta\n            fields = line.split()\n            print(fields[0], fields[1])\n\n            # Cargamos la imagen y obtenemos su etiqueta\n            image = cv.imread(fields[0], cv.IMREAD_GRAYSCALE)\n            label = fields[1]\n\n            # TODO: Extraemos los keypoints de la imagen y guardamos los descriptores en la variable desc\n            desc = None\n\n            #\u00a0A\u00f1adimos los descriptores y la etiqueta\n            descriptors.append(desc)\n            labels.append(label)\n\n    return descriptors,labels\n\n\n# Funci\u00f3n que recibe descriptores y etiquetas de train y test.\n#\u00a0Hace un matching asignando a cada muestra de test la etiqueta de la muestra de train m\u00e1s cercana y calcula la tasa de acierto\ndef testORB(descTrain, labelsTrain, descTest, labelsTest):\n\n    print('Matching')\n\n    matcher = cv.BFMatcher(cv.NORM_HAMMING)\n\n    ok = 0.0\n\n    for dtest, ltest in zip(descTest,labelsTest):\n        #\u00a0En bestLabel guardamos la etiqueta de la imagen m\u00e1s similar que aparece en el conjunto de train (tienes que calcularlo debajo)\n        bestLabel = None\n\n        for dtrain, ltrain in zip(descTrain, labelsTrain):\n\n            if dtrain is not None and dtest is not None:\n\n                #\u00a0TODO:  Solo consideramos que dos puntos son similares si su distancia es menor o igual a 90.\n                # La imagen mas similar sera la que tiene m\u00e1s keypoints coincidentes. Hay que extraer su etiqueta y guardarla en \"bestLabel\" (ahora pone 'cat' pero deber\u00edas modificarlo)\n                bestLabel = 'cat'\n\n        if bestLabel == ltest:\n            ok += 1\n\n    print('Accuracy=', ok/len(labelsTest))\n\n    return 0\n\n# Programa principal\ndef main(args):\n\n    if args.test:\n        #\u00a0Calculamos los descriptores de test\n        descTest, labelsTest = readData('test.txt')\n\n        #\u00a0Cargamos los descriptores de train ya calculados en la fase anterior\n        with open('trainData.dat','rb') as storedDescriptors:\n            data = pickle.load(storedDescriptors)\n\n        # Comparamos los descriptores de train y test\n        descTrain = data[0]\n        labelsTrain = data[1]\n        testORB(descTrain, labelsTrain, descTest, labelsTest)\n\n    else:\n        # Calculamos los descriptores de train\n        descTrain, labelsTrain = readData('train.txt')\n\n        # Guardamos los descriptores de train en un fichero para poder usarlos en la fase de test\n        dataTrain=(descTrain, labelsTrain)\n        with open('trainData.dat','wb') as storedDescriptors:\n            pickle.dump(dataTrain, storedDescriptors)\n\n    return 0\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description = 'Programa para reconocer objetos usando descriptores ORB.')\n    parser.add_argument('--test', action = 'store_true')     #\u00a0Si se indica test ser\u00e1 true, si no se indica entonces se asume train\n    args = parser.parse_args()\n    main(args)\n</code></pre> <p>Para ejecutar el programa, debes hacerlo sin opciones (para entrenamiento) o con la opci\u00f3n <code>test</code> para validaci\u00f3n. Ejemplo de uso para entrenamiento:</p> <pre><code>python orbBF.py\n</code></pre> <p>Para test: <pre><code>python orbBF.py --test\n</code></pre></p> <p>Una vez completes el c\u00f3digo, la primera fase es ejecutar extraer el fichero de caracter\u00edsticas ORB de todas las im\u00e1genes que hay en el fichero <code>train.txt</code>. Despu\u00e9s se puede ejecutar en modo <code>test</code> para reconocer todas las im\u00e1genes del conjunto de test y compararlas con las etiquetas del conjunto <code>train</code>.</p> <p>El resultado final tras la fase de test deber\u00eda ser el siguiente:</p> <pre><code>Accuracy= 0.24\n</code></pre>"},{"location":"reconocimiento.html#ejercicio_1","title":"Ejercicio","text":"<p>Con el ejercicio anterior hemos hecho una primera aproximaci\u00f3n para reconocer objetos pero, como hemos visto en teor\u00eda, es mucho m\u00e1s eficiente entrenar un clasificador espec\u00edfico para nuestros datos. De esta forma no es necesario consultar todas las im\u00e1genes de entrenamiento para buscar la imagen m\u00e1s cercana como se hace con vecinos m\u00e1s cercanos.</p> <p>En la asignatura no hemos visto c\u00f3mo funcionan internamente los algoritmos de aprendizaje supervisado, pero usaremos uno en modo \"caja negra\" para mejorar el porcentaje de acierto que hemos obtenido en el ejercicio anterior.</p> <p>Partimos de este otro c\u00f3digo, que como ver\u00e1s tiene partes comunes con el anterior. Gu\u00e1rdalo con el nombre <code>HOGSVM.py</code> y compl\u00e9talo con los comentarios marcados con <code>TODO</code>:</p> <pre><code>import cv2 as cv\nimport argparse\nimport numpy as np\n\nlabelNames = ['book', 'cat', 'chair', 'dog', 'glass', 'laptop', 'pen', 'remote', 'cellphone', 'tv']\n\n#\u00a0Funci\u00f3n que lee un fichero de texto y calcula sus descriptores HOG\n# Devuelve los descriptores y las etiquetas le\u00eddas\ndef extractHOGFeatures(filename):\n\n    print('Extrayendo descriptores HOG')\n\n    hog = cv.HOGDescriptor()\n\n    descriptors = list()\n    labels = list()\n\n    with open(filename, 'r') as f:\n        for line in f.readlines():\n\n            #\u00a0Separamos nombre y etiqueta\n            fields = line.split()\n            print(fields[0], fields[1])\n\n            #\u00a0Leemos la imagen y obtenemos su etiqueta\n            image = cv.imread(fields[0], cv.IMREAD_GRAYSCALE)\n            label = fields[1]\n\n            # Extraemos el descriptor HOG\n            # TODO: Para que todas las imagenes tengan el mismo tama\u00f1o de descriptor, las debemos reescalar a 128x128\n            # TODO: Ahora debemos calcular el descriptor HOG con un stride de 128x128 (asumimos que el objeto ocupa toda la imagen) y un padding (0,0).\n            # TODO: A\u00f1adimos el descriptor obtenido de la imagen al vector descriptors, y la etiqueta al vector labels\n\n    return descriptors, labels\n\n\n# https://answers.opencv.org/question/183596/how-exactly-does-bovw-work-for-python-3-open-cv3/\ndef trainSVM(descriptors, labels):\n    print('Entrenando SVM')\n\n    # Configuramos el clasificador SVM (lo guardamos en la variable svm)\n    svm = cv.ml.SVM_create()\n\n    #\u00a0TODO: Debemos indicar que el clasificador es de tipo C_SVC\n    # TODO: Su kernel debe ser LINEAR\n    # TODO: El criterio de finalizaci\u00f3n debe ser MAX_ITER con 100 iteraciones m\u00e1ximas y EPS=1e-5.\n    # Ayuda para los puntos anteriores: https://docs.opencv.org/4.12.0/d1/d73/tutorial_introduction_to_svm.html\n\n    # Convertimos las etiquetas a valores num\u00e9ricos (necesario para entrenar SVM con la librer\u00eda de OpenCV)\n    labelsIndex = list()\n    for label in labels:\n        labelsIndex.append(labelNames.index(label))\n\n    #\u00a0Entrenamos el modelo SVM\n    svm.train(np.array(descriptors, dtype='float32'), cv.ml.ROW_SAMPLE, np.array(labelsIndex, dtype='int'))\n\n    #\u00a0Guardamos el modelo entrenado\n    svm.save('modelSVM.xml')\n\n\n#\u00a0Funci\u00f3n para evaluar los resultados\ndef testHOGSVM(descriptors, labels):\n\n    #\u00a0Cargamos el modelo\n    svm = cv.ml.SVM_load('modelSVM.xml')\n\n    #\u00a0Clasificamos\n    npDescriptors = np.array(descriptors, dtype='float32')\n    results = svm.predict(npDescriptors)\n\n    # Calculamos el resultado\n    ok = 0.0\n    i = 0\n    for pred in results[1]:\n        if pred[0] == labelNames.index(labels[i]):\n            ok += 1\n        i += 1\n\n    print('Accuracy=', ok/len(labels)) \n\n\ndef main(args):\n\n    if args.test:\n        # Calculamos los descriptores\n        descriptors, labels = extractHOGFeatures('test.txt')\n\n        # Hacemos la comparaci\u00f3n entre los descriptores de train y de test\n        testHOGSVM(descriptors,labels)\n\n    else:\n        # Calculamos los descriptores\n        descriptors, labels = extractHOGFeatures('train.txt')\n\n        # Entrenamos y guardamos el modelo\n        trainSVM(descriptors, labels)\n\n    return 0\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description = 'Reconocimineto de objetos usando descriptores HOG y SVM')\n    parser.add_argument('--test', action = 'store_true')     #\u00a0Si se indica \"test\" args.test ser\u00e1 true, si no se indica entonces se asume train\n    args = parser.parse_args()\n    main(args)\n</code></pre> <p>En este caso se trata de extraer los descriptores HOG de las im\u00e1genes del conjunto <code>train</code> para entrenar un clasificador supervisado de tipo Support Vector Machine (SVM). Una vez entrenado el modelo, se guardar\u00e1 en el fichero <code>modelSVM.xml</code>. De esta forma, en la la fase de reconocimiento (<code>test</code>) se cargar\u00e1 el modelo para predecir la clase de una imagen desconocida, en lugar de comparar la imagen con todas las del conjunto de entrenamiento.</p> <p>El resultado tras comprobar el conjunto de test debe ser este:</p> <pre><code>Accuracy= 0.37\n</code></pre> <p>Como ves, esta t\u00e9cnica, adem\u00e1s de ser mucho m\u00e1s r\u00e1pida, mejora claramente el resultado respecto al ejercicio anterior.</p>"},{"location":"reconocimiento.html#reconocimiento-con-redes-neuronales","title":"Reconocimiento con redes neuronales","text":"<p>En el ejercicio anterior, en lugar de HOG podr\u00edamos haber usado caracter\u00edsticas neuronales extraidas de una de las \u00faltimas capas de una red convolucional de la forma que vimos aqu\u00ed. Si tienes curiosidad puedes probarlo, ver\u00e1s como el porcentaje de acierto mejora significativamente. </p> <p>Adicionalmente OpenCV tambi\u00e9n incorpora muchos ejemplos de clasificaci\u00f3n usando redes neuronales para tareas como reconocimiento de caras, texto, etc. Es recomendable echar un vistazo a estos ejemplos porque probablemente te ayuden para el proyecto. </p>"},{"location":"segmentacion.html","title":"Tema 5- Procesamiento de imagen: Segmentaci\u00f3n","text":"<p>En este tema veremos c\u00f3mo segmentar im\u00e1genes, detectando los p\u00edxeles pertenecientes a los objetos de inter\u00e9s.</p>"},{"location":"segmentacion.html#umbralizacion","title":"Umbralizaci\u00f3n","text":"<p>Como puede verse en este enlace, OpenCV proporciona varios m\u00e9todos para realizar umbralizaci\u00f3n b\u00e1sica mediante la funci\u00f3n <code>threshold</code>. Esta funci\u00f3n tambi\u00e9n implementa la umbralizaci\u00f3n de Otsu indicando como par\u00e1metro <code>cv.THRES_OTSU</code>:</p> <pre><code>dst, th = cv.threshold(img, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n</code></pre> <p>La umbralizaci\u00f3n adaptativa se implementa usando la funci\u00f3n adaptiveThreshold.</p> <p>El m\u00e9todo de Chow-Kaneko no est\u00e1 en OpenCV aunque no ser\u00eda complicado de implementar.</p>"},{"location":"segmentacion.html#contornos","title":"Contornos","text":"<p>Tal como hemos visto en teor\u00eda, podemos usar un algoritmo de detecci\u00f3n de bordes para poder estimar posteriormente los contornos de los objetos (y de esta forma segmentarlos). En OpenCV existe una funci\u00f3n para realizar esta tarea llamada <code>findContours</code> que s\u00f3lo puede usarse para extraer contornos a partir de los bordes detectados con otro algoritmo (es decir, trabaja con una imagen binaria como entrada). En este enlace puedes ver un ejemplo de un programa que usa <code>findContours</code> y posteriormente la funci\u00f3n <code>drawContours</code> para dibujar el resultado usando colores aleatorios.</p> <p> </p> <p>A continuaci\u00f3n podemos ver un ejemplo de sintaxis de <code>findContours</code>:</p> <pre><code>contours, hierarchy = cv.findContours(image, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)\n</code></pre> <p>Esta funci\u00f3n devuelve una lista de contornos detectados en la imagen junto con su jerarqu\u00eda. La jerarqu\u00eda hace referencia a la relaci\u00f3n de los contornos entre s\u00ed, ya que a veces tenemos unos contornos dentro de otros (en ese caso, los primeros ser\u00e1n \"hijos\" de los segundos, que son los contornos \"padre\"). </p> <p>El segundo par\u00e1metro de esta funci\u00f3n es el tipo de algoritmo usado para devolver los contornos. El m\u00e9todo m\u00e1s sencillo es <code>cv.RETR_LIST</code>, que devuelve simplemente un listado e ignora la jerarqu\u00eda. Alternativamente se puede usar, por ejemplo, <code>cv.RETR_TREE</code>, que contiene la jerarqu\u00eda completa. </p> <p>El tercer par\u00e1metro es el m\u00e9todo de aproximaci\u00f3n. En el caso de usar <code>cv.CHAIN_APPROX_NONE</code> se devuelven todos los puntos del contorno, pero como esto es bastante ineficiente para algunos algoritmos (como veremos en el siguiente tema), a veces se usan t\u00e9cnicas de reducci\u00f3n de puntos para simplificar los contornos, por ejemplo usando la opci\u00f3n <code>cv.CHAIN_APPROX_SIMPLE</code>.</p>"},{"location":"segmentacion.html#crecimiento-y-division-de-regiones","title":"Crecimiento y divisi\u00f3n de regiones","text":"<p>OpenCV no tiene ning\u00fan m\u00e9todo de crecimiento de regiones, aunque existen algunos ejemplos de c\u00f3digo que lo implementan siguiendo la metodolog\u00eda que hemos visto en teor\u00eda. Tampoco existen m\u00e9todos de divisi\u00f3n y uni\u00f3n, pero en este enlace puedes consultar un ejemplo sencillo.</p>"},{"location":"segmentacion.html#watershed","title":"Watershed","text":"<p>En OpenCV est\u00e1 implementado el algoritmo <code>Watershed</code>. Puedes ver ejemplos de uso de un programa interactivo y tambi\u00e9n no interactivo, es decir, deduciendo de forma autom\u00e1tica los marcadores iniciales.</p> <p>En el ejemplo no interactivo se segmenta la siguiente imagen:</p> <p> </p> <p>En este otro enlace puedes encontrar un ejemplo de Watershed que usa la webcam en tiempo real.</p>"},{"location":"segmentacion.html#clustering","title":"Clustering","text":"<p>El algoritmo Mean-shift est\u00e1 implementado en la librer\u00eda <code>sklearn</code> para uso general de clustering de datos, aunque tambi\u00e9n puede encontrarse en la librer\u00eda de OpenCV. </p> <p>La librer\u00eda sklearn (en realidad <code>scikit</code>) es la m\u00e1s usada en python para algoritmos de aprendizaje autom\u00e1tico tradicional, y la utilizan muchos programas que tambi\u00e9n usan OpenCV.</p> <p>En este \u00faltimo caso (usando OpenCV) tenemos dos opciones: El m\u00e9todo <code>meanshift</code>, que suele usarse para tracking (como veremos en el tema de v\u00eddeo), o <code>pyrMeanShiftFiltering</code>, que se usa directamente para segmentar im\u00e1genes en color:</p> <pre><code>dst = cv.pyrMeanShiftFiltering(img, 25, 60)\n</code></pre> <p>En este caso, el segundo par\u00e1metro de la funci\u00f3n (25) es el radio de la ventana espacial, y el segundo (60) el radio de la ventana de color. La segmentaci\u00f3n de esta funci\u00f3n es piramidal, es decir, se hace a distintas resoluciones y se combinan los resultados. A continuaci\u00f3n se muestra una imagen de entrada y el resultado obtenido.</p> <p> </p> <p>El algoritmo k-means se implementa en OpenCV mediante la funci\u00f3n <code>kmeans</code>. En este enlace puedes ver un ejemplo de uso. Tal como hemos visto en teor\u00eda, a nivel pr\u00e1ctico la principal diferencia con Mean-shift es que con k-means debemos indicar el n\u00famero de clusters K, mientras que con mean-shift no podemos indicar la cantidad de elementos distintos que queremos encontrar.</p> <p></p> <p>Este algoritmo tambi\u00e9n puede encontrarse para uso general en la librer\u00eda <code>sklearn</code>. </p>"},{"location":"segmentacion.html#metodos-basados-en-grafos","title":"M\u00e9todos basados en grafos","text":"<p>El m\u00e9todo basado en grafos m\u00e1s com\u00fan en OpenCV es <code>GrabCut</code>. Puedes ver un ejemplo de esta funci\u00f3n usada de forma interactiva en este enlace.</p> <p></p>"},{"location":"segmentacion.html#metodos-de-saliency","title":"M\u00e9todos de saliency","text":"<p>OpenCV implementa algunos algoritmos de <code>saliency</code>, entre los que se encuentra <code>Spectral Residual</code>. Este algoritmo es sencillo y tambi\u00e9n se puede implementar a mano, pero a continuaci\u00f3n puedes ver un ejemplo que usa la implementaci\u00f3n de OpenCV basado en el c\u00f3digo de este enlace:</p> <pre><code>import cv2 as cv\nimport argparse\nimport numpy as np\n\nparser = argparse.ArgumentParser(description = 'Programa para calcular Meanshift')\nparser.add_argument('--imagen', '-i', type=str, default='giraffe.jpg')\nargs = parser.parse_args()\n\n#\u00a0Cargamos la imagen\nimg = cv.imread(args.imagen)\n\n# Comprobamos que la imagen se ha podido leer\nif img is None:\n    print('Error al cargar la imagen')\n    quit()\n\n#\u00a0Calculamos el saliency map\nsaliency = cv.saliency.StaticSaliencySpectralResidual_create()\n(success, saliencyMap) = saliency.computeSaliency(img)\n\n#\u00a0Convertimos el resultado (float32) a una imagen uint8\nsaliencyMap = (saliencyMap * 255).astype('uint8')\n\n# Umbralizamos para obtener una imagen binaria\nbinaryMap = cv.threshold(saliencyMap, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)[1]\n\n#\u00a0Mostramos los resultados\ncv.imshow('original', img)\ncv.imshow('saliency', saliencyMap)\ncv.imshow('binary', binaryMap)\n\ncv.waitKey(0)\n</code></pre> <p></p> <p></p> <p></p> <p>La mayor\u00eda de m\u00e9todos recienten que estiman una funci\u00f3n <code>saliency</code> suelen ser bastante m\u00e1s complejos y se basan en t\u00e9cnicas de aprendizaje autom\u00e1tico. Si quieres ver un ejemplo de otro m\u00e9todo puedes mirar el algoritmo GMR, que est\u00e1 basado en grafos (no usa aprendizaje autom\u00e1tico).</p> <p></p>"},{"location":"segmentacion.html#ejercicio","title":"Ejercicio","text":"<p>Como podemos ver en la siguiente imagen, tenemos un robot TrimBot y queremos usarlo para podar rosales: </p> <p></p> <p>El primer paso para que el robot haga su trabajo es detectar las ramas principales usando las c\u00e1maras que equipa. En este ejercicio vamos a intentar resolver esta tarea. </p> <p>Para ello disponemos de una serie de im\u00e1genes sint\u00e9ticas ya etiquetadas con la posici\u00f3n de las ramas principales. Por tanto, se trata de un problema de segmentaci\u00f3n binaria: dada una imagen como la siguiente, el objetivo es identificar los p\u00edxeles que pertenecen a estas ramas (segunda imagen). </p> <p> </p> <p>Si te fijas ver\u00e1s que las ramas muy finas no aparecen marcadas en la imagen segmentada ya que no son de inter\u00e9s para la poda. Sin embargo, ten en cuenta que aunque tu programa las detecte no hay problema y no cambiar\u00e1 mucho el resultado porque en realidad son pocos p\u00edxeles.</p> <p>Llamaremos a nuestro programa <code>roses.py</code>. Debe recibir por par\u00e1metro la imagen de entrada y el fichero donde se almacenar\u00e1 el resultado de la segmentaci\u00f3n:</p> <pre><code>parser = argparse.ArgumentParser(description = 'Programa para segmentar tallos de rosales')\nparser.add_argument('--entrada', '-i', type=str)\nparser.add_argument('--salida', '-o', type=str)\nargs = parser.parse_args()\n</code></pre> <ul> <li><code>entrada</code> es la imagen de entrada.</li> <li><code>salida</code> es el nombre del fichero en el que vamos a guardar el resultado de la segmentaci\u00f3n, que ser\u00e1 una imagen en escala de grises (en blanco los p\u00edxeles que pertenecen a las ramas y en negro los que no).</li> </ul> <p>Se proporciona el programa de evaluaci\u00f3n y una serie de im\u00e1genes de entrada junto con sus correspondientes anotaciones para comprobar los resultados.</p> <p>Para comenzar, descarga todos los materiales de este ejercicio que se encuentran en el fichero roses.zip.</p> <p>Cuando descomprimas este archivo, podr\u00e1s ver en el directorio <code>roses</code> las siguientes carpetas:</p> <ul> <li><code>input</code>: im\u00e1genes de entrada que debe segmentar tu algoritmo. Estas im\u00e1genes son un subconjunto sint\u00e9tico de la base de datos ROSeS.</li> <li><code>output</code>: directorio inicialmente vac\u00edo donde se guardar\u00e1n los resultados de segmentaci\u00f3n de tu m\u00e9todo.</li> <li><code>gt</code>: im\u00e1genes correctamente etiquetadas para evaluar los resultados del programa.</li> </ul> <p>En el directorio principal tambi\u00e9n hay un programa <code>evaluate.py</code> que se usar\u00e1 para evaluar los resultados. Este programa implementa una figura de m\u00e9rito denominada intersecci\u00f3n sobre la uni\u00f3n de dos im\u00e1genes (del ingl\u00e9s Intersection over Union, normalmente abreviado como IoU). En otras palabras, esta m\u00e9trica devuelve cu\u00e1nto se parece una imagen obtenida y una segmentada, siendo su rango posible [0,1] con el valor 1 representando una segmentaci\u00f3n perfecta.</p> <p>El script <code>evaluate.py</code> tiene dos modos de funcionamiento, seg\u00fan el argumento <code>-m</code> que se le facilite:</p> <ul> <li>Modo imagen individual: Modo que utilizaremos si queremos calcular la figura de m\u00e9rito sobre una \u00fanica imagen, la cual se especifica en el argumento <code>-i</code>. Su sintaxis es:</li> </ul> <pre><code>python evaluate.py -m single -i input/imagen.png\n</code></pre> <ul> <li>Modo batch: Modo que utilizaremos para evaluar una colecci\u00f3n de im\u00e1genes contenidas en una carpeta (por defecto, la carpeta <code>input</code>, aunque se puede cambiar utilizando el argumento <code>-i</code>). Su sintaxis es:</li> </ul> <pre><code>python evaluate.py -m batch [-i input]\n</code></pre> <p>N\u00f3tese que este programa requiere internamente el programa <code>roses.py</code> que hay que desarrollar en esta pr\u00e1ctica (<code>evaluate.py</code> no \u00fanicamente calcula la m\u00e9trica sino que tambi\u00e9n llama a <code>roses.py</code> para que se realice la segmentaci\u00f3n). Por ello deber\u00e9is de implementar este programa para que el script de evaluaci\u00f3n funcione.</p> <p>Por \u00faltimo, cabe destacar que, para resolver este problema, puedes usar cualquier t\u00e9cnica que hayamos visto en la asignatura. Finalmente, la nota de este ejercicio ser\u00e1 m\u00e1s alta cuanto mayor sea el valor de la Media IoU.</p>"},{"location":"transformaciones.html","title":"Tema 3- Procesamiento de imagen: Transformaciones","text":"<p>En este tema comenzaremos a modificar im\u00e1genes mediante transformaciones de varios tipos.</p>"},{"location":"transformaciones.html#transformaciones-puntuales","title":"Transformaciones puntuales","text":"<p>Como hemos visto anteriormente, en OpenCV se pueden realizar operaciones directas con matrices mediante la librer\u00eda <code>numpy</code>. Por ejemplo, podemos multiplicar por 4  todos los p\u00edxeles de una imagen esta forma:</p> <pre><code>dst = src * 4\n</code></pre> <p>Tal como puedes ver, en las operaciones aritm\u00e9ticas se pueden usar indistintamente tanto n\u00fameros como arrays o matrices. </p> <p>Adem\u00e1s de las operaciones aritm\u00e9ticas b\u00e1sicas (suma, resta, multiplicaci\u00f3n y divisi\u00f3n), tambi\u00e9n podemos usar AND, OR, XOR y NOT mediante las siguientes funciones de <code>numpy</code>:</p> <pre><code>dst = np.bitwise_and(src1, src2)\ndst = np.bitwise_or(src2, src2)\ndst = np.bitwise_xor(src1, src2)\ndst = np.bitwise_not(src1) #\u00a0Alternativa: dst = np.invert(src1)\n</code></pre> <p>Por ejemplo, para invertir una imagen (transformar lo blanco a negro y lo negro a blanco) podemos usar la instrucci\u00f3n <code>bitwise_not</code>. Este m\u00e9todo es un alias de <code>np.invert</code>.</p> <p>Ecualizar histogramas en escala de grises es muy sencillo con la funci\u00f3n <code>equalizeHist</code>:</p> <pre><code>equ = cv.equalizeHist(img)\n</code></pre> <p>Tambi\u00e9n podemos umbralizar una imagen en escala de grises mediante la funci\u00f3n <code>threshold</code>, obteniendo como resultado una imagen binaria (tambi\u00e9n llamada m\u00e1scara) que puede resaltar informaci\u00f3n relevante para una tarea determinada. La umbralizaci\u00f3n consiste en poner a 0 los p\u00edxeles que tienen un valor inferior al umbral indicado y es la forma m\u00e1s b\u00e1sica de realizar segmentaci\u00f3n (como veremos en detalle en el tema 5). Ejemplo de llamada a <code>threshold</code>:</p> <pre><code># Ponemos a 0 los p\u00edxeles cuyos valores est\u00e9n por debajo de 128, y a 255 los que est\u00e9n por encima\nth, dst = cv.threshold(src, 128, 255, cv.THRESH_BINARY) \n</code></pre> <p>El \u00faltimo par\u00e1metro es el tipo de umbralizaci\u00f3n. En OpenCV tenemos 5 tipos de umbralizaci\u00f3n que pueden consultarse aqu\u00ed, aunque el valor m\u00e1s usado es <code>cv.THRESH_BINARY</code> (umbralizaci\u00f3n binaria).</p> <p>Este m\u00e9todo s\u00f3lo funciona con im\u00e1genes en escala de grises. Para umbralizar im\u00e1genes en color, OpenCV ofrece la funci\u00f3n <code>inrange</code>. Dada una imagen en 3 canales, esta funci\u00f3n devuelve otra imagen de un canal con aquellos p\u00edxeles que est\u00e1n en un determinado rango coloreados en blanco, y los que quedan fuera del mismo en negro. Por tanto, puede usarse para realizar una segmentaci\u00f3n b\u00e1sica por color, tal como veremos en detalle en el tema 5.</p> <pre><code># Dejamos en blanco los p\u00edxeles que est\u00e1n entre (0,10,20) y (40,40,51)\ndst = cv.inRange(src, (0, 10, 20), (40, 40, 51)) \n</code></pre> <p>En OpenCV existen t\u00e9cnicas alternativas de binarizaci\u00f3n como el umbralizado adaptativo o el m\u00e9todo de Otsu, que tambi\u00e9n veremos en el tema de segmentaci\u00f3n porque no se pueden considerar transformaciones puntuales al tener en cuenta los valores de intensidad de los p\u00edxeles vecinos.</p>"},{"location":"transformaciones.html#ejercicio","title":"Ejercicio","text":"<p>Haz un programa llamado <code>ecualizar.py</code> que realice una ecualizaci\u00f3n de histograma, como el que hace la funci\u00f3n <code>equalizeHist</code> de OpenCV, pero de forma manual. El algoritmo para ecualizar un histograma puede consultarse en las transparencias de teor\u00eda.</p> <p>Para resolver este ejercicio puedes partir del siguiente esqueleto de c\u00f3digo, completando las partes que se indican con <code>TODO</code>. </p> <pre><code>import cv2 as cv\nimport numpy as np\nimport argparse\nfrom collections import Counter # Necesario para el acumulador\n\n# Gesti\u00f3n de par\u00e1metros\nparser = argparse.ArgumentParser(description='Programa para ecualizar un histograma (sin usar calcHist).')\nparser.add_argument('--imagen', '-i', type=str, default = 'flor.jpg')\nparser.add_argument('--salida', '-r', type=str, default = 'florEq.jpg')\nargs = parser.parse_args()\n\n# Cargamos la imagen\nimg = cv.imread(args.imagen, cv.IMREAD_GRAYSCALE)\n\n#img = np.array([52, 55, 61, 62, 59, 55, 63, 62, 55]) # Puedes descomentar esto para comprobar si el resultado es correcto usando los datos de ejemplo de teor\u00eda\n\n# Comprobamos que la imagen se ha podido leer\nif img is None:\n    print(\"No se ha podido abrir la imagen\", args.imagen)\n    quit()\n\n#\u00a01- Calculamos el histograma. Para esto, en lugar de usar calcHist creamos un vector x con los \n#    valores \u00fanicos de los p\u00edxeles, y otro vector h con la cantidad de elementos para cada valor.\nx, h  = np.unique(img, return_counts=True)\n\n#\u00a02- Calculamos la CDF (la guardamos en c).\nc = np.cumsum(h)\n\n#\u00a03- TODO: Ya tenemos x, h y c. Ahora debemos calcular x', que guardamos en la variable xp.\nxp = None\n\n#\u00a04- TODO: Establecemos el nuevo valor de cada p\u00edxel (es decir, cambiamos en la imagen todos los valores x por los valores xp)\n\n#\u00a05- TODO: Guardamos la imagen resultante en el fichero indicado en args.salida\n</code></pre> <p>Usando como entrada la siguiente imagen:</p> <p></p> <p>La salida deber\u00eda ser:</p> <p></p>"},{"location":"transformaciones.html#transformaciones-globales","title":"Transformaciones globales","text":"<p>Una de las transformaciones globales m\u00e1s usadas en imagen es la transformada de Fourier. En OpenCV tenemos la funci\u00f3n <code>dft</code> que calcula esta transformada, aunque necesitamos hacer un preproceso para preparar la entrada a esta funci\u00f3n, y un postproceso para calcular la magnitud y la fase a partir de su resultado. En Visi\u00f3n por Computador no entraremos en detalles sobre c\u00f3mo usar la transformada de Fourier en OpenCV, pero si quieres saber m\u00e1s puedes consultar este enlace.</p>"},{"location":"transformaciones.html#transformaciones-afines","title":"Transformaciones afines","text":"<p>En OpenCV la mayor\u00eda de transformaciones geom\u00e9tricas se implementan creando una matriz de transformaci\u00f3n y aplic\u00e1ndola a la imagen original con <code>warpAffine</code>.</p> <p>Esta funci\u00f3n requiere como entrada una matriz de tama\u00f1o 2x3, ya que implementa las transformaciones afines mediante matrices aumentadas. Como hemos visto en teor\u00eda, la \u00faltima fila de la matriz aumentada en una transformaci\u00f3n af\u00edn es siempre (0,0,1) por lo que no hay que indicarla (por este motivo se indica una matriz de 2x3 en lugar de 3x3).</p> <p>La funci\u00f3n <code>warpAffine</code> tiene tambi\u00e9n par\u00e1metros para indicar el tipo de interpolaci\u00f3n (<code>flags</code>) y el comportamiento en los bordes, tal como puede verse en su documentaci\u00f3n.</p> <p>En general, podemos usar <code>warpAffine</code> para implementar cualquier transformaci\u00f3n af\u00edn. Por ejemplo, podr\u00edamos implementar la siguiente traslaci\u00f3n...</p> <p></p> <p>...con este c\u00f3digo:</p> <pre><code>import cv2 as cv\nimport numpy as np\n\nimg = cv.imread('lena.jpg', cv.IMREAD_GRAYSCALE)\n\n# Valores de translaci\u00f3n\ntx = 100\nty = 50\n\n# Definimos la matriz\nM = np.float32([[1, 0, tx],\n                [0, 1, ty]]) \n\n# El par\u00e1metro flags puede omitirse, por defecto es INTER_LINEAR          \nrows, cols = img.shape\ndst = cv.warpAffine(img, M, (cols, rows), flags=cv.INTER_CUBIC)\n\ncv.imshow('traslacion', dst)\ncv.waitKey(0)\n</code></pre> <p>Alternativamente a usar las matrices de transformaci\u00f3n af\u00edn con <code>warpAffine</code> existen funciones espec\u00edficas para ayudar a gestionar las transformaciones de rotaci\u00f3n, reflexi\u00f3n y escalado como vamos a ver a continuaci\u00f3n:</p>"},{"location":"transformaciones.html#rotacion","title":"Rotaci\u00f3n","text":"<p>La rotaci\u00f3n sobre un \u00e1ngulo se define con la siguiente matriz de transformaci\u00f3n:</p> <p></p> <p>Sin embargo, OpenCV tambi\u00e9n permite rotar indicando un centro de rotaci\u00f3n ajustable para poder usar cualquier punto de referencia como eje. Para esto se usa la funci\u00f3n <code>getRotationMatrix2D</code>, que recibe como primer par\u00e1metro el eje de rotaci\u00f3n:</p> <pre><code>rows, cols = img.shape\n\n#\u00a0Obtenemos la matriz de rotaci\u00f3n con 90 grados usando como referencia el centro de la imagen \nM = cv.getRotationMatrix2D((cols/2,rows/2), 90, 1) #\u00a0El \u00faltimo par\u00e1metro (1) es la escala\ndst = cv.warpAffine(img, M, (cols,rows))\n</code></pre>"},{"location":"transformaciones.html#reflexion","title":"Reflexi\u00f3n","text":"<p>Existe una funci\u00f3n espec\u00edfica (<code>flip</code>) que implementa la reflexi\u00f3n sin necesidad de usar <code>warpAffine</code>.</p> <pre><code>flipVertical = cv.flip(img, 0)\n</code></pre> <p>El tercer par\u00e1metro de <code>flip</code> puede ser 0 (reflexi\u00f3n sobre el eje x), positivo (por ejemplo, 1 es reflexi\u00f3n sobre el eje y), o negativo (por ejemplo, -1 es sobre los dos ejes).</p>"},{"location":"transformaciones.html#escalado","title":"Escalado","text":"<p>El escalado tambi\u00e9n se implementa mediante una funci\u00f3n espec\u00edfica llamada <code>resize</code>, que permite indicar unas dimensiones concretas o una proporci\u00f3n entre la imagen origen y destino.</p> <pre><code># 1- Especificando un tama\u00f1o determinado (en este ejemplo, 20x30):\ndim = (20, 30)\ndst = cv.resize(src, dim, interpolation = cv.INTER_LINEAR) # El \u00faltimo par\u00e1metro de interpolaci\u00f3n es opcional\n\n# 2- Especificando una escala, por ejemplo el 75% de la imagen original:\ndst = cv.resize(src, (0,0), fx=0.75, fy=0.75, cv.INTER_LINEAR) # El \u00faltimo par\u00e1metro de interpolaci\u00f3n es opcional\n</code></pre>"},{"location":"transformaciones.html#transformaciones-proyectivas","title":"Transformaciones proyectivas","text":"<p>Como hemos visto en teor\u00eda, la transformaci\u00f3n proyectiva no es af\u00edn, por lo que no conserva el paralelismo de las l\u00edneas de la imagen original.</p> <p>Para hacer una transformaci\u00f3n proyectiva debemos indicar una matriz de 3x3 y usar la funci\u00f3n <code>warpPerspective</code>. Por ejemplo:</p> <pre><code># Definimos la matriz\nM = np.float32([[1, 0, 0],\n                [0.5, 1, 0],\n                [0.2, 0, 1]])\n\n#\u00a0Implementamos la transformaci\u00f3n proyectiva\nrows, cols = img.shape\ndst = cv.warpPerspective(img, M, (cols, rows))\n</code></pre> <p>La lista completa de par\u00e1metros de esta funci\u00f3n puede verse en este enlace.</p> <p>Tambi\u00e9n tenemos otra opci\u00f3n muy pr\u00e1ctica para implementar una transformaci\u00f3n de este tipo, ya que suele ser muy complicado estimar a priori los valores de la matriz para realizar una transformaci\u00f3n concreta. Esta alternativa consiste en proporcionar dos arrays de 4 puntos (siendo cada punto un vector de dos dimensiones que representa las coordenadas del mismo en el plano XY): El primero ser\u00e1 de la imagen original, y el segundo contiene la proyecci\u00f3n de esos puntos (d\u00f3nde van a quedar finalmente) en la imagen destino. Con estos datos podemos usar <code>getPerspectiveTransform</code> para calcular los valores de la matriz de transformaci\u00f3n.</p> <pre><code># Los dos par\u00e1metros que recibe getPerspectiveTransform deben ser arrays de puntos, y cada punto es un array de dos elementos float. \nM = cv.getPerspectiveTransform(input_pts, output_pts)\n\n# Aplicamos la transformacion usando interpolaci\u00f3n lineal. Los valores widthDst y heightDst indican el tama\u00f1o de la imagen destino.\ndst = cv.warpPerspective(src, M, (widthDst, heightDst), flags=cv.INTER_LINEAR)\n</code></pre> <p>Siendo un ejemplo de vector <code>input_pts</code> el que sigue: <pre><code>input_pts = np.float32([[120, 13], [610, 24], [2, 491], [622, 500]])\n</code></pre></p>"},{"location":"transformaciones.html#transformaciones-en-entorno-de-vecindad","title":"Transformaciones en entorno de vecindad","text":"<p>En esta secci\u00f3n veremos c\u00f3mo implementar transformaciones en entorno de vecindad usando OpenCV, en particular convoluciones y filtros de mediana.</p>"},{"location":"transformaciones.html#filtros-de-convolucion","title":"Filtros de convoluci\u00f3n","text":"<p>Las convoluciones se implementan con la funci\u00f3n <code>filter2D</code>.</p> <p></p> <p>Esta funci\u00f3n recibe los siguientes par\u00e1metros:</p> <ul> <li><code>src</code>: Imagen de entrada</li> <li><code>ddepth</code>: Resoluci\u00f3n radiom\u00e9trica (depth) de la matriz <code>dst</code>. Un valor negativo indica que la resoluci\u00f3n es la misma que tiene la imagen de entrada.</li> <li><code>kernel</code>: El kernel a convolucionar con la imagen.</li> <li><code>anchor</code> (opcional): La posici\u00f3n de anclaje del kernel (como puede verse en la figura) relativa a su origen. El punto (-1,-1) indica el centro del kernel (es el valor por defecto).</li> <li><code>delta</code> (opcional): Un valor para a\u00f1adir a cada p\u00edxel durante la convoluci\u00f3n. Por defecto, 0.</li> <li><code>borderType</code> (opcional): El m\u00e9todo a seguir en los bordes de la imagen para interpolaci\u00f3n, ya que en estos puntos el filtro se sale de la imagen. Puede ser <code>cv.BORDER_REPLICATE</code>, <code>cv.BORDER_REFLECT</code>, <code>cv.BORDER_REFLECT_101</code>, <code>cv.BORDER_WRAP</code>,  <code>cv.BORDER_CONSTANT</code>, o <code>cv.BORDER_DEFAULT</code> (que es el valor por defecto).</li> </ul> <p>Ejemplos de llamadas a esta funci\u00f3n:</p> <pre><code># Esta forma es la m\u00e1s habitual\ndst = cv.filter2D(src, -1, kernel) \n# Indicando qu\u00e9 hacer en los bordes\ndst = cv.filter2D(src, -1, kernel, borderType=cv.BORDER_CONSTANT)\n</code></pre> <p>Evidentemente hay que crear antes un kernel para convolucionarlo con la imagen. Por ejemplo, podr\u00eda ser el siguiente:</p> <pre><code>kernel = numpy.array([[-1, -1, -1, -1, -1],\n                      [-1, -1, -1, -1, -1],\n                      [-1, -1, 24, -1, -1],\n                      [-1, -1, -1, -1, -1],\n                      [-1, -1, -1, -1, -1]])\n</code></pre> <p>Pregunta: \u00bfQu\u00e9 tipo de filtro acabamos de crear?</p>"},{"location":"transformaciones.html#filtro-de-mediana","title":"Filtro de mediana","text":"<p>El filtro de mediana se implementa de forma muy sencilla en OpenCV:</p> <pre><code>dst = cv.medianBlur(src, 5)\n</code></pre> <p>El \u00faltimo par\u00e1metro indica el tama\u00f1o del kernel, que siempre ser\u00e1 cuadrado (en este ejemplo, 5x5 p\u00edxeles).</p>"},{"location":"transformaciones.html#transformaciones-morfologicas","title":"Transformaciones morfol\u00f3gicas","text":"<p>OpenCV proporciona una serie de funciones predefinidas para realizar transformaciones morfol\u00f3gicas.</p>"},{"location":"transformaciones.html#erosion-y-dilatacion","title":"Erosi\u00f3n y dilataci\u00f3n","text":"<p>La sintaxis de estas operaciones morfol\u00f3gicas b\u00e1sicas es sencilla:</p> <pre><code>dst = cv.erode(src, element)\ndst = cv.dilate(src, element)\n</code></pre> <p>Ambas funciones necesitan un elemento estructurante, llamado <code>element</code> en el c\u00f3digo anterior. Al igual que en el caso de <code>filter2D</code> se pueden a\u00f1adir opcionalmente los par\u00e1metros <code>anchor</code>, <code>delta</code> y <code>borderType</code>.</p> <p>Para crear el elemento estructurante se usa la funci\u00f3n <code>getStructuringElement</code>:</p> <pre><code># Forma del filtro\nerosion_type = cv.MORPH_ELLIPSE \n\n#\u00a0El \u00faltimo par\u00e1metro es el tama\u00f1o del filtro, en este caso 5x5\nelement = cv.getStructuringElement(erosion_type, (5,5)) \n</code></pre> <p>El elemento estructurante puede tener forma de caja (<code>MORPH_RECT</code>), de cruz (<code>MORPH_CROSS</code>) o de elipse (<code>MORPH_ELLIPSE</code>).</p>"},{"location":"transformaciones.html#apertura-cierre-y-top-hat","title":"Apertura, cierre y Top-Hat","text":"<p>El resto de funciones de transformaci\u00f3n morfol\u00f3gica se implementan mediante la funci\u00f3n <code>morphologyEx</code>. Por ejemplo:</p> <pre><code>dst = cv.morphologyEx(src, cv.MORPH_OPEN, element)\n</code></pre> <p>Esta funci\u00f3n se invoca con los mismos par\u00e1metros que <code>erode</code> o <code>dilate</code> a\u00f1adiendo el par\u00e1metro que indica el tipo de operaci\u00f3n:</p> <ul> <li>Apertura: <code>MORPH_OPEN</code></li> <li>Cierre: <code>MORPH_CLOSE</code></li> <li>Gradiente: <code>MORPH_GRADIENT</code></li> <li>White Top Hat: <code>MORPH_TOPHAT</code></li> <li>Black Top Hat: <code>MORPH_BLACKHAT</code></li> </ul> <p>En este enlace puedes ver c\u00f3digo de ejemplo para implementar un interfaz que permite probar estas operaciones modificando sus par\u00e1metros.</p>"},{"location":"transformaciones.html#ejercicio_1","title":"Ejercicio","text":"<p>Implementa un programa llamado <code>detectarFichas.py</code> que cargue la siguiente imagen <code>damas.jpg</code>, corrija la perspectiva del tablero y detecte las fichas blancas y rojas.</p> <p></p> <p>Los par\u00e1metros del programa deben ser los siguientes:</p> <pre><code>parser = argparse.ArgumentParser(description='Programa para obtener la posici\u00f3n de las damas')\nparser.add_argument('--imagen', '-i', type=str, default = 'damas.jpg')\nparser.add_argument('--salidaPerspectiva', '-p', type=str, default = 'corrected.jpg')\nparser.add_argument('--salidaRojas', '-r', type=str, default = 'rojas.jpg')\nparser.add_argument('--salidaBlancas', '-b', type=str, default = 'blancas.jpg')\n</code></pre> <p>Primero vamos a corregir la perspectiva. Para esto se proporcionan los 4 puntos de las esquinas del tablero en la imagen original:</p> <pre><code>278, 27  # Esquina superior izquierda\n910, 44  # Esquina superior derecha\n27, 546  # Esquina inferior izquierda\n921, 638 # Esquina inferior derecha\n</code></pre> <p>El programa debe aplicar una transformaci\u00f3n proyectiva y guardar el resultado en otra imagen de tama\u00f1o 640x640 p\u00edxeles cuyo nombre se ha pasado por par\u00e1metro (por defecto, <code>corrected.jpg</code>). La imagen resultado deber\u00eda ser como la siguiente:</p> <p></p> <p>A continuaci\u00f3n se detallan los pasos para obtener las fichas rojas y blancas a partir de esta imagen.</p>"},{"location":"transformaciones.html#fichas-rojas","title":"Fichas rojas","text":"<p>A continuaci\u00f3n se muestra el resultado de detectar las fichas rojas:</p> <p></p> <p>Para realizar esta detecci\u00f3n el programa debe seguir los siguientes pasos:</p> <ul> <li>Realizar una umbralizaci\u00f3n qued\u00e1ndonos s\u00f3lo con los p\u00edxeles que tengan un color dentro de un rango BGR entre (0,0,50) y (40,30,255). Podemos visualizar el resultado con <code>imshow</code>. Deber\u00edamos tener los p\u00edxeles de las fichas rojas resaltados, aunque la detecci\u00f3n es todav\u00eda imperfecta y existen huecos.</li> <li>Crear un elemento estructurante circular de tama\u00f1o 10x10 p\u00edxeles y aplicar un operador de cierre para perfilar mejor los contornos de las fichas y eliminar estos huecos.</li> <li>Guardar la imagen resultante en el fichero pasado por par\u00e1metro (por defecto, <code>rojas.jpg</code>). Deber\u00eda dar el mismo resultado que se muestra en la imagen anterior.</li> </ul>"},{"location":"transformaciones.html#fichas-blancas","title":"Fichas blancas","text":"<ul> <li>Ahora debes intentar resaltar s\u00f3lo las fichas blancas lo mejor que puedas, guardando el resultado en el fichero <code>blancas.jpg</code>. Para esto puedes usar filtrado de color (en cualquier espacio, como HSV) y realizar transformaciones morfol\u00f3gicas o de cualquier otro tipo. Probablemente no te salga demasiado bien pero es un problema mucho m\u00e1s complicado que la detecci\u00f3n de las fichas rojas al confundirse el color de las damas con el de las casillas blancas. </li> </ul>"}]}