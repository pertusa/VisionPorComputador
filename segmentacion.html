
<!doctype html>
<html lang="es" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>Tema 5- Procesamiento de imagen: Segmentación - Visión por Computador</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.2a3383ac.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="Teal" data-md-color-accent="Teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tema-5-procesamiento-de-imagen-segmentacion" class="md-skip">
          Saltar a contenido
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Cabecera">
    <a href="." title="Visión por Computador" class="md-header__button md-logo" aria-label="Visión por Computador" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Visión por Computador
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Tema 5- Procesamiento de imagen: Segmentación
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Búsqueda" placeholder="Búsqueda" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Buscar">
        
        <button type="reset" class="md-search__icon md-icon" title="Limpiar" aria-label="Limpiar" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Inicializando búsqueda
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navegación" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="Visión por Computador" class="md-nav__button md-logo" aria-label="Visión por Computador" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Visión por Computador
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="install.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Instalación de OpenCV
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="intro.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T1- Introducción
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="imagenvideo.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T2- Imagen digital y vídeo
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="transformaciones.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T3- Transformaciones
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="deteccion.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T4- Detección
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Tabla de contenidos">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Tabla de contenidos
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#umbralizacion" class="md-nav__link">
    <span class="md-ellipsis">
      Umbralización
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contornos" class="md-nav__link">
    <span class="md-ellipsis">
      Contornos
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#crecimiento-y-division-de-regiones" class="md-nav__link">
    <span class="md-ellipsis">
      Crecimiento y división de regiones
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#watershed" class="md-nav__link">
    <span class="md-ellipsis">
      Watershed
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#clustering" class="md-nav__link">
    <span class="md-ellipsis">
      Clustering
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metodos-basados-en-grafos" class="md-nav__link">
    <span class="md-ellipsis">
      Métodos basados en grafos
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metodos-de-saliency" class="md-nav__link">
    <span class="md-ellipsis">
      Métodos de saliency
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ejercicio" class="md-nav__link">
    <span class="md-ellipsis">
      Ejercicio
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="tema-5-procesamiento-de-imagen-segmentacion">Tema 5- Procesamiento de imagen: Segmentación<a class="headerlink" href="#tema-5-procesamiento-de-imagen-segmentacion" title="Permanent link">&para;</a></h1>
<p>En este tema veremos cómo segmentar imágenes, detectando los píxeles pertenecientes a los objetos de interés.</p>
<h2 id="umbralizacion">Umbralización<a class="headerlink" href="#umbralizacion" title="Permanent link">&para;</a></h2>
<!---
Pedir que implementen el "Algoritmo sencillo"?
-->

<p>Como puede verse en <a href="https://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html">este enlace</a>, OpenCV proporciona varios métodos para realizar umbralización básica mediante la función <code>threshold</code>. Esta función también implementa la umbralización de <strong>Otsu</strong> indicando como parámetro <code>cv.THRES_OTSU</code>:</p>
<div class="highlight"><pre><span></span><code><span class="n">dst</span><span class="p">,</span> <span class="n">th</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="o">+</span><span class="n">cv</span><span class="o">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>
</code></pre></div>
<p>La umbralización <strong>adaptativa</strong> se implementa usando la función <a href="https://www.geeksforgeeks.org/python-thresholding-techniques-using-opencv-set-2-adaptive-thresholding/">adaptiveThreshold</a>.</p>
<p>El método de <strong>Chow-Kaneko</strong> no está en OpenCV aunque no sería complicado de implementar.</p>
<h2 id="contornos">Contornos<a class="headerlink" href="#contornos" title="Permanent link">&para;</a></h2>
<p>Tal como hemos visto en teoría, podemos usar un algoritmo de detección de bordes para poder estimar posteriormente los <strong>contornos</strong> de los objetos (y de esta forma segmentarlos). En OpenCV existe una función para realizar esta tarea llamada <code>findContours</code> que sólo puede usarse para extraer contornos a partir de los bordes detectados con otro algoritmo (es decir, trabaja con una imagen binaria como entrada). En <a href="https://docs.opencv.org/master/d4/d73/tutorial_py_contours_begin.html">este enlace</a> puedes ver un ejemplo de un programa que usa <code>findContours</code> y posteriormente la función <code>drawContours</code> para dibujar el resultado usando colores aleatorios.</p>
<p><img alt="Contornos" src="images/segmentacion/contours_input.jpg" /> <img alt="Contornos" src="images/segmentacion/contours_output.jpg" /></p>
<p>A continuación podemos ver un ejemplo de <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=moments#findcontours">sintaxis</a> de <code>findContours</code>:</p>
<div class="highlight"><pre><span></span><code><span class="n">contours</span><span class="p">,</span> <span class="n">hierarchy</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">findContours</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">RETR_LIST</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">CHAIN_APPROX_NONE</span><span class="p">)</span>
</code></pre></div>
<p>Esta función devuelve una lista de contornos detectados en la imagen junto con su jerarquía. La <a href="https://docs.opencv.org/master/d9/d8b/tutorial_py_contours_hierarchy.html">jerarquía</a> hace referencia a la relación de los contornos entre sí, ya que a veces tenemos unos contornos dentro de otros (en ese caso, los primeros serán "hijos" de los segundos, que son los contornos "padre"). </p>
<p>El segundo parámetro de esta función es el tipo de algoritmo usado para devolver los contornos. El método más sencillo es <code>cv.RETR_LIST</code>, que devuelve simplemente un listado e ignora la jerarquía. Alternativamente se puede usar, por ejemplo, <code>cv.RETR_TREE</code>, que contiene la jerarquía completa. </p>
<p>El tercer parámetro es el método de aproximación. En el caso de usar <code>cv.CHAIN_APPROX_NONE</code> se devuelven todos los puntos del contorno, pero como esto es bastante ineficiente para algunos algoritmos (como veremos en el siguiente tema), a veces se usan <a href="https://docs.opencv.org/master/d4/d73/tutorial_py_contours_begin.html">técnicas de reducción de puntos</a> para simplificar los contornos, por ejemplo usando la opción <code>cv.CHAIN_APPROX_SIMPLE</code>.</p>
<h2 id="crecimiento-y-division-de-regiones">Crecimiento y división de regiones<a class="headerlink" href="#crecimiento-y-division-de-regiones" title="Permanent link">&para;</a></h2>
<p>OpenCV no tiene ningún método de <strong>crecimiento de regiones</strong>, aunque existen algunos <a href="https://github.com/Panchamy/RegionGrowing">ejemplos de código</a> que lo implementan siguiendo la metodología que hemos visto en teoría. Tampoco existen métodos de <strong>división y unión</strong>, pero en <a href="http://vgg.fiit.stuba.sk/2016-06/split-and-merge/">este enlace</a> puedes consultar un ejemplo sencillo.</p>
<!---
http://www.lengrand.fr/2011/11/simple-region-growing-implementation-in-python
-->

<h2 id="watershed">Watershed<a class="headerlink" href="#watershed" title="Permanent link">&para;</a></h2>
<p>En OpenCV está implementado el algoritmo <code>Watershed</code>. Puedes ver ejemplos de uso de un programa <a href="https://github.com/opencv/opencv/blob/master/samples/python/watershed.py">interactivo</a> y también <a href="https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_watershed/py_watershed.html">no interactivo</a>, es decir, deduciendo de forma automática los marcadores iniciales.</p>
<!---
https://github.com/callaunchpad/OpenCV-Samples/blob/master/watershed.py
--->

<p>En el ejemplo no interactivo se segmenta la siguiente imagen:</p>
<p><img alt="entrada watershed" src="images/segmentacion/water_coins.jpg" /> <img alt="salida watershed" src="images/segmentacion/water_result.jpg" /></p>
<p>En este <a href="https://github.com/callaunchpad/OpenCV-Samples/blob/master/watershed.py">otro enlace</a> puedes encontrar un ejemplo de Watershed que usa la webcam en tiempo real.</p>
<h2 id="clustering">Clustering<a class="headerlink" href="#clustering" title="Permanent link">&para;</a></h2>
<!---
https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_video/py_meanshift/py_meanshift.html
---->

<p>El algoritmo <strong>Mean-shift</strong> está implementado en la librería <code>sklearn</code> para <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html">uso general de clustering de datos</a>, aunque también puede encontrarse en la librería de OpenCV. </p>
<blockquote>
<p>La librería <a href="https://scikit-learn.org/stable/">sklearn</a> (en realidad <code>scikit</code>) es la más usada en python para algoritmos de aprendizaje automático tradicional, y la utilizan muchos programas que también usan OpenCV.</p>
</blockquote>
<p>En este último caso (usando OpenCV) tenemos dos opciones: El <a href="https://docs.opencv.org/master/d7/d00/tutorial_meanshift.html">método <code>meanshift</code></a>, que suele usarse para <em>tracking</em> (como veremos en el tema de vídeo), o <code>pyrMeanShiftFiltering</code>, que se usa directamente para segmentar imágenes en color:</p>
<div class="highlight"><pre><span></span><code><span class="n">dst</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">pyrMeanShiftFiltering</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
</code></pre></div>
<p>En este caso, el segundo parámetro de la función (25) es el radio de la ventana espacial, y el segundo (60) el radio de la ventana de color. La segmentación de esta función es piramidal, es decir, se hace a distintas resoluciones y se combinan los resultados. A continuación se muestra una imagen de entrada y el resultado obtenido.</p>
<!---
se implementa medianta la función `meanShift`, cuya ayuda puedes consultar [aquí](https://docs.opencv.org/3.0-alpha/modules/imgproc/doc/filtering.html#pyrmeanshiftfiltering). Aquí puedes ver un ejemplo de código que usa este algoritmo:

<div class="highlight"><pre><span></span><code><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;opencv2/opencv.hpp&gt;</span><span class="c1"> // Incluimos OpenCV</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>

<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">cv</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">std</span><span class="p">;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="w">   </span><span class="n">Mat</span><span class="w"> </span><span class="n">src</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">imread</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
<span class="w">   </span><span class="n">Mat</span><span class="w"> </span><span class="n">dst</span><span class="p">;</span>

<span class="w">   </span><span class="n">TermCriteria</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">TermCriteria</span><span class="o">::</span><span class="n">MAX_ITER</span><span class="p">,</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">   </span><span class="n">pyrMeanShiftFiltering</span><span class="p">(</span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="mi">25</span><span class="p">,</span><span class="w"> </span><span class="mi">60</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">);</span>
<span class="w">   </span><span class="n">imwrite</span><span class="p">(</span><span class="s">&quot;pills2.png&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">dst</span><span class="p">);</span>

<span class="w">   </span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>
--->

<p><img alt="entrada meanshift" src="images/segmentacion/pills1.png" />
<img alt="salida meanshift" src="images/segmentacion/pills2.png" /></p>
<!---

La **segmentación Mean-shift** es algo distinta al filtrado, ya que segmenta la imagen en regiones que tienen aproximadamente el mismo color. Por tanto, mapea cada píxel con su segmento correspondiente. Para obtener contornos de objetos debes usar la segmentación Mean-shift.

**Mean-shift** también está implementado en OpenCV mediante la función
https://github.com/daviddoria/Examples/blob/master/c%2B%2B/OpenCV/MeanShiftSegmentation/MeanShiftSegmentation.cxx
https://stackoverflow.com/questions/31429342/difference-between-meanshiftfiltering-and-meanshiftsegmentation-in-opencv
http://answers.opencv.org/question/175486/meanshift-sample-code-in-c/
https://stackoverflow.com/questions/4831813/image-segmentation-using-mean-shift-explained

void pyrMeanShiftFiltering(InputArray src, OutputArray dst, double sp, double sr, int maxLevel = 1, TermCriteria termcrit = TermCriteria (TermCriteria::MAX_ITER + TermCriteria::EPS, 5, 1)): This implements the  filtering stage of the mean-shift segmentation, obtaining an image, dst, with color gradients and  ne-grain texture  attened. The sp and sr parameters indicate the spatial window and the color window radii.

<!---
Para retina, spatialRad=4; colorRad=8; maxPyrLevel=1

Para retina también:
http://www.pittnuts.com/2015/12/image-segmentation-by-opencv/
--->

<p>El algoritmo <strong>k-means</strong> se implementa en OpenCV mediante la función <code>kmeans</code>. En <a href="https://docs.opencv.org/master/d1/d5c/tutorial_py_kmeans_opencv.html">este enlace</a> puedes ver un ejemplo de uso. Tal como hemos visto en teoría, a nivel práctico la principal diferencia con <em>Mean-shift</em> es que con <em>k-means</em> debemos indicar el número de clusters <em>K</em>, mientras que con <em>mean-shift</em> no podemos indicar la cantidad de elementos distintos que queremos encontrar.</p>
<p><img alt="k-means" src="images/segmentacion/kmeans.jpg" /></p>
<p>Este algoritmo también puede encontrarse para <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">uso general</a> en la librería <code>sklearn</code>. </p>
<h2 id="metodos-basados-en-grafos">Métodos basados en grafos<a class="headerlink" href="#metodos-basados-en-grafos" title="Permanent link">&para;</a></h2>
<p>El método basado en grafos más común en OpenCV es <code>GrabCut</code>. Puedes ver un ejemplo de esta función usada de forma interactiva en <a href="https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html">este enlace</a>.</p>
<p><img alt="grabcut" src="images/segmentacion/grabcut.jpg" /></p>
<h2 id="metodos-de-saliency">Métodos de saliency<a class="headerlink" href="#metodos-de-saliency" title="Permanent link">&para;</a></h2>
<p>OpenCV implementa algunos algoritmos de <code>saliency</code>, entre los que se encuentra <code>Spectral Residual</code>. Este algoritmo es sencillo y también se puede implementar a mano, pero a continuación puedes ver un ejemplo que usa la implementación de OpenCV basado en el código de <a href="https://www.cronj.com/blog/finding-region-of-interest-roi-saliency/">este enlace</a>:</p>
<!----
https://www.cronj.com/blog/finding-region-of-interest-roi-saliency/
---->
<!---
> TODO: EXPLICAR FINEGRAINED QUE APARECE EN ESE EJEMPLO? FUNCIONA MUCHO MEJOR QUE SR!
--->

<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">cv2</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cv</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span> <span class="o">=</span> <span class="s1">&#39;Programa para calcular Meanshift&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--imagen&#39;</span><span class="p">,</span> <span class="s1">&#39;-i&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;giraffe.jpg&#39;</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># Cargamos la imagen</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">imagen</span><span class="p">)</span>

<span class="c1"># Comprobamos que la imagen se ha podido leer</span>
<span class="k">if</span> <span class="n">img</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error al cargar la imagen&#39;</span><span class="p">)</span>
    <span class="n">quit</span><span class="p">()</span>

<span class="c1"># Calculamos el saliency map</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">saliency</span><span class="o">.</span><span class="n">StaticSaliencySpectralResidual_create</span><span class="p">()</span>
<span class="p">(</span><span class="n">success</span><span class="p">,</span> <span class="n">saliencyMap</span><span class="p">)</span> <span class="o">=</span> <span class="n">saliency</span><span class="o">.</span><span class="n">computeSaliency</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="c1"># Convertimos el resultado (float32) a una imagen uint8</span>
<span class="n">saliencyMap</span> <span class="o">=</span> <span class="p">(</span><span class="n">saliencyMap</span> <span class="o">*</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>

<span class="c1"># Umbralizamos para obtener una imagen binaria</span>
<span class="n">binaryMap</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">saliencyMap</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">THRESH_BINARY</span> <span class="o">|</span> <span class="n">cv</span><span class="o">.</span><span class="n">THRESH_OTSU</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Mostramos los resultados</span>
<span class="n">cv</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;original&#39;</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>
<span class="n">cv</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;saliency&#39;</span><span class="p">,</span> <span class="n">saliencyMap</span><span class="p">)</span>
<span class="n">cv</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span> <span class="n">binaryMap</span><span class="p">)</span>

<span class="n">cv</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p><img alt="entrada saliency" src="images/segmentacion/giraffe.jpg" /></p>
<p><img alt="Saliency map" src="images/segmentacion/saliency_map.jpg" /></p>
<p><img alt="Binary map" src="images/segmentacion/binary_map.jpg" /></p>
<p>La mayoría de métodos recienten que estiman una función <code>saliency</code> suelen ser bastante más complejos y se basan en técnicas de aprendizaje automático. Si quieres ver un ejemplo de otro método puedes mirar <a href="https://github.com/ruanxiang/mr_saliency">el algoritmo GMR</a>, que está basado en grafos (no usa aprendizaje automático).</p>
<p><img alt="GMR" src="images/segmentacion/gmr.png" /></p>
<!---
Saliency? BING implementado fuera de contrib, creo (probar en el lab!->NO ES SEGMENTACION!
http://docs.opencv.org/3.2.0/d8/d65/group__saliency.html
https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp
-->

<!---
Segmentación (TODO)
http://www.bogotobogo.com/python/OpenCV_Python/python_opencv3_Image_Global_Thresholding_Adaptive_Thresholding_Otsus_Binarization_Segmentations.php

http://docs.opencv.org/2.4/doc/tutorials/imgproc/threshold/threshold.html UMBRALIZACION ADAPTATIVA!!
Add Otsu’s Binarization: http://docs.opencv.org/3.2.0/d7/d4d/tutorial_py_thresholding.html-->

<!---
<div class="highlight"><pre><span></span><code><span class="n">findContours</span>
</code></pre></div>

http://acodigo.blogspot.com.es/2016/04/seguimiento-de-objetos-por-color.html
Se puede con este código (findContours) marcar el contorno y buscar el centroide de las piezas de las damas. También está la función drawContours. findContours implementa el algoritmo http://download.xuebalib.com/xuebalib.com.17233.pdf basado en detección de bordes.
-->

<hr />
<h2 id="ejercicio">Ejercicio<a class="headerlink" href="#ejercicio" title="Permanent link">&para;</a></h2>
<!----
JAVI!: https://rua.ua.es/dspace/bitstream/10045/103541/5/2020_Cuevas-Velasquez_etal_ComputElectronAgricult_preprint.pdf
  Descarga datos: https://drive.google.com/file/d/1MVHF3OZPQo53m8v3cE0u_Zyb8OdeNmpj/view?usp=sharing

Alternativas
Manzanas: https://conservancy.umn.edu/handle/11299/206575 (no me gusta, hay muchos errores de etiquetado!)
Flores (Mejor el dataset de peras): https://data.nal.usda.gov/dataset/data-multi-species-fruit-flower-detection-using-refined-semantic-segmentation-network
Limones: https://github.com/softwaremill/lemon-dataset
Grietas: https://www.irit.fr/~Sylvie.Chambon/Crack_Detection_Database.html
More: https://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm

PERAS:
Imágenes: https://data.nal.usda.gov/system/files/Pear_1.zip
Etiquetas: https://data.nal.usda.gov/system/files/PearLabels_2.zip
-->

<p>Como podemos ver en la siguiente imagen, tenemos un robot <a href="http://trimbot2020.webhosting.rug.nl">TrimBot</a> y queremos usarlo para podar rosales: </p>
<p><img alt="Robot" src="images/segmentacion/robotRoses.png" /></p>
<p>El primer paso para que el robot haga su trabajo es detectar las ramas principales usando las cámaras que equipa. En este ejercicio vamos a intentar resolver esta tarea. </p>
<p>Para ello disponemos de una serie de imágenes sintéticas ya etiquetadas con la posición de las ramas principales. Por tanto, se trata de un problema de segmentación binaria: dada una imagen como la siguiente, el objetivo es identificar los píxeles que pertenecen a estas ramas (segunda imagen). </p>
<p><img alt="Ejemplo de imagen de entrada" src="images/segmentacion/4.png" />
<img alt="Ejemplo de segmentación sin errores" src="images/segmentacion/4gt.png" /></p>
<p>Si te fijas verás que las ramas muy finas no aparecen marcadas en la imagen segmentada ya que no son de interés para la poda. Sin embargo, ten en cuenta que aunque tu programa las detecte no hay problema y no cambiará mucho el resultado porque en realidad son pocos píxeles.</p>
<p>Llamaremos a nuestro programa <code>roses.py</code>. Debe recibir por parámetro la imagen de entrada y el fichero donde se almacenará el resultado de la segmentación:</p>
<div class="highlight"><pre><span></span><code><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span> <span class="o">=</span> <span class="s1">&#39;Programa para segmentar tallos de rosales&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--entrada&#39;</span><span class="p">,</span> <span class="s1">&#39;-i&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--salida&#39;</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</code></pre></div>
<ul>
<li><code>entrada</code> es la imagen de entrada.</li>
<li><code>salida</code> es el nombre del fichero en el que vamos a guardar el resultado de la segmentación, que será una imagen en escala de grises (en blanco los píxeles que pertenecen a las ramas y en negro los que no).</li>
</ul>
<p>Se proporciona el <em>script</em> de evaluación y una serie de imágenes de entrada junto con sus correspondientes anotaciones para comprobar los resultados.</p>
<p>Para comenzar, descarga todos los materiales de este ejercicio que se encuentran en el fichero <a href="images/segmentacion/roses.zip">roses.zip</a>.</p>
<p>Cuando descomprimas este archivo, podrás ver en el directorio <code>roses</code> las siguientes carpetas:</p>
<ul>
<li><code>input</code>: imágenes de entrada que debe segmentar tu algoritmo. Estas imágenes son un subconjunto sintético de la base de datos <a href="https://rua.ua.es/dspace/bitstream/10045/103541/5/2020_Cuevas-Velasquez_etal_ComputElectronAgricult_preprint.pdf">ROSeS</a>.</li>
<li><code>output</code>: directorio inicialmente vacío donde se guardarán los resultados de segmentación de tu método.</li>
<li><code>gt</code>: imágenes correctamente etiquetadas para evaluar los resultados del programa.</li>
</ul>
<p>En el directorio principal también hay un programa <code>evaluate.py</code> que se usará para evaluar los resultados. Este programa implementa una figura de mérito denominada intersección sobre la unión de dos imágenes (del inglés Intersection over Union, normalmente abreviado como IoU). En otras palabras, esta métrica devuelve cuánto se parece una imagen obtenida y una segmentada, siendo su rango posible [0,1] con el valor 1 representando una segmentación perfecta.</p>
<p>El script <code>evaluate.py</code> tiene dos modos de funcionamiento, según el argumento <code>-m</code> que se le facilite:</p>
<ul>
<li>Modo imagen individual: Modo que utilizaremos si queremos calcular la figura de mérito sobre una única imagen, la cual se especifica en el argumento <code>-i</code>. Su sintaxis es:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">python</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">m</span> <span class="n">single</span> <span class="o">-</span><span class="n">i</span> <span class="nb">input</span><span class="o">/</span><span class="n">imagen</span><span class="o">.</span><span class="n">png</span>
</code></pre></div>
<ul>
<li>Modo batch: Modo que utilizaremos para evaluar una colección de imágenes contenidas en una carpeta (por defecto, la carpeta <code>input</code>, aunque se puede cambiar utilizando el argumento <code>-i</code>). Su sintaxis es:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">python</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">m</span> <span class="n">batch</span> <span class="p">[</span><span class="o">-</span><span class="n">i</span> <span class="nb">input</span><span class="p">]</span>
</code></pre></div>
<p>Nótese que este script requiere internamente el programa <code>roses.py</code> que debéis desarrollar en esta práctica (<code>evaluate.py</code> no únicamente calcula la figura de mérito sino que también llama a <code>roses.py</code> para que se realice la segmentación). Por ello deberéis de implementar este programa para que el script de evaluación funcione.</p>
<p>Por último, cabe destacar que, para resolver este problema, puedes usar cualquier técnica que hayamos visto en la asignatura. Finalmente, la nota de este ejercicio será más alta cuanto mayor sea el valor de la Media IoU.</p>
<!---
Para los algoritmos de umbralización:
http://www.isi.uu.nl/Research/Databases/DRIVE
Métrica: Intersection over union: https://stackoverflow.com/questions/11262312/opencv-intersection-between-two-binary-images

Para el resto:
https://docs.opencv.org/3.0-rc1/d3/d2d/group__datasets__is.html
-->

<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": ["content.code.copy"], "search": "assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copiado al portapapeles", "clipboard.copy": "Copiar al portapapeles", "search.result.more.one": "1 m\u00e1s en esta p\u00e1gina", "search.result.more.other": "# m\u00e1s en esta p\u00e1gina", "search.result.none": "No se encontraron documentos", "search.result.one": "1 documento encontrado", "search.result.other": "# documentos encontrados", "search.result.placeholder": "Teclee para comenzar b\u00fasqueda", "search.result.term.missing": "Falta", "select.version": "Seleccionar versi\u00f3n"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>